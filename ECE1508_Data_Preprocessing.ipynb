{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epcdolO1Tlw5"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eithwTf8Tp6-"
      },
      "source": [
        "### Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W_pCAnFf4jnu"
      },
      "outputs": [],
      "source": [
        "# Time-aware Leave-One-Out (LOO) split for MovieLens ratings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_ts_unit(ts_series):\n",
        "    vmax = float(ts_series.max())\n",
        "    return \"ms\" if vmax > 1e12 else \"s\"\n",
        "\n",
        "def time_aware_loo_split(\n",
        "    ratings_csv: str,\n",
        "    out_dir: str,\n",
        "    rating_threshold: float = 4.0,\n",
        "    min_positives: int = 2,\n",
        "    also_csv: bool = False,\n",
        "):\n",
        "    out = Path(out_dir); (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
        "    ratings = pd.read_csv(ratings_csv)\n",
        "\n",
        "    # 1) Basic checks\n",
        "    need = {\"userId\",\"movieId\",\"rating\",\"timestamp\"}\n",
        "    missing = need - set(ratings.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
        "\n",
        "    # 2) Normalize timestamp and keep implicit positives\n",
        "    unit = _detect_ts_unit(ratings[\"timestamp\"])\n",
        "    ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
        "\n",
        "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
        "    # Drop duplicate (user,item) -> keep earliest\n",
        "    pos = pos.sort_values([\"userId\",\"ts\",\"movieId\"], kind=\"mergesort\")\n",
        "    pos = pos.drop_duplicates([\"userId\",\"movieId\"], keep=\"first\")\n",
        "\n",
        "    # Keep users with at least min_positives\n",
        "    pos = pos[pos.groupby(\"userId\")[\"movieId\"].transform(\"size\") >= min_positives].copy()\n",
        "\n",
        "    # 3) Rank by time per user, assign splits: last=test, second last=val (if >=3), rest=train\n",
        "    pos[\"n\"] = pos.groupby(\"userId\")[\"movieId\"].transform(\"size\")\n",
        "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
        "    pos[\"split\"] = \"train\"\n",
        "    pos.loc[pos[\"idx\"] == pos[\"n\"]-1, \"split\"] = \"test\"\n",
        "    pos.loc[(pos[\"n\"]>=3) & (pos[\"idx\"] == pos[\"n\"]-2), \"split\"] = \"val\"\n",
        "\n",
        "    train = pos[pos[\"split\"]==\"train\"][[\"userId\",\"movieId\",\"ts\"]].reset_index(drop=True)\n",
        "    val_targets  = pos[pos[\"split\"]==\"val\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"val_item\",\"ts\":\"ts_val\"}).reset_index(drop=True)\n",
        "    test_targets = pos[pos[\"split\"]==\"test\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"test_item\",\"ts\":\"ts_test\"}).reset_index(drop=True)\n",
        "\n",
        "    # 4) Build ID maps from TRAIN only (contiguous 0..U-1 and 0..I-1)\n",
        "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"])\n",
        "    uids[\"uid\"] = range(len(uids))\n",
        "    iids = pd.DataFrame(sorted(train[\"movieId\"].unique()), columns=[\"movieId\"])\n",
        "    iids[\"iid\"] = range(len(iids))\n",
        "\n",
        "    # 5) Also provide indexed versions (useful for MF/ALS)\n",
        "    train_idx = (train.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                      .merge(iids, on=\"movieId\", how=\"inner\"))\n",
        "    val_idx = None\n",
        "    if len(val_targets):\n",
        "        val_idx = (val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                              .merge(iids, left_on=\"val_item\", right_on=\"movieId\", how=\"left\")\n",
        "                              .drop(columns=[\"movieId\"]))\n",
        "    test_idx = (test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                               .merge(iids, left_on=\"test_item\", right_on=\"movieId\", how=\"left\")\n",
        "                               .drop(columns=[\"movieId\"]))\n",
        "\n",
        "    # 6) Save outputs\n",
        "    sp = out / \"splits\"\n",
        "    train.to_parquet(sp / \"train.parquet\", index=False)\n",
        "    if len(val_targets):\n",
        "        val_targets.to_parquet(sp / \"val_targets.parquet\", index=False)\n",
        "    test_targets.to_parquet(sp / \"test_targets.parquet\", index=False)\n",
        "    uids.to_parquet(sp / \"user_id_map.parquet\", index=False)\n",
        "    iids.to_parquet(sp / \"item_id_map.parquet\", index=False)\n",
        "    train_idx.to_parquet(sp / \"train_indexed.parquet\", index=False)\n",
        "    if val_idx is not None:\n",
        "        val_idx.to_parquet(sp / \"val_targets_indexed.parquet\", index=False)\n",
        "    test_idx.to_parquet(sp / \"test_targets_indexed.parquet\", index=False)\n",
        "\n",
        "    if also_csv:\n",
        "        train.to_csv(sp / \"train.csv\", index=False)\n",
        "        if len(val_targets): val_targets.to_csv(sp / \"val_targets.csv\", index=False)\n",
        "        test_targets.to_csv(sp / \"test_targets.csv\", index=False)\n",
        "        uids.to_csv(sp / \"user_id_map.csv\", index=False)\n",
        "        iids.to_csv(sp / \"item_id_map.csv\", index=False)\n",
        "        train_idx.to_csv(sp / \"train_indexed.csv\", index=False)\n",
        "        if val_idx is not None:\n",
        "            val_idx.to_csv(sp / \"val_targets_indexed.csv\", index=False)\n",
        "        test_idx.to_csv(sp / \"test_targets_indexed.csv\", index=False)\n",
        "\n",
        "    # 7) Quick stats + cold-start counts\n",
        "    cold_val = int(val_idx[\"iid\"].isna().sum()) if val_idx is not None and \"iid\" in val_idx else 0\n",
        "    cold_test = int(test_idx[\"iid\"].isna().sum()) if \"iid\" in test_idx else 0\n",
        "    stats = f\"\"\"Time-aware LOO split summary\n",
        "Users (TRAIN map): {len(uids)}\n",
        "Items (TRAIN map): {len(iids)}\n",
        "TRAIN positives  : {len(train)}\n",
        "VAL users        : {len(val_targets[\"userId\"].unique()) if len(val_targets) else 0}\n",
        "TEST users       : {len(test_targets[\"userId\"].unique())}\n",
        "Cold-start VAL items  : {cold_val}\n",
        "Cold-start TEST items : {cold_test}\n",
        "\"\"\"\n",
        "    (sp / \"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
        "    print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThfMPG-5Txll"
      },
      "source": [
        "### Call Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boIFiP3OAAgz",
        "outputId": "94890907-82a2-42a9-8279-cff31decd1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time-aware LOO split summary\n",
            "Users (TRAIN map): 200466\n",
            "Items (TRAIN map): 54711\n",
            "TRAIN positives  : 15537362\n",
            "VAL users        : 200143\n",
            "TEST users       : 200466\n",
            "Cold-start VAL items  : 221\n",
            "Cold-start TEST items : 257\n",
            "\n"
          ]
        }
      ],
      "source": [
        "time_aware_loo_split(\n",
        "    ratings_csv=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\ratings.csv\",\n",
        "    out_dir=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\",\n",
        "    rating_threshold=4.0,   # implicit “like” threshold\n",
        "    min_positives=2,        # drop users with <2 positives\n",
        "    also_csv=True           # optional CSV mirrors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-rj0cuAT4t8"
      },
      "source": [
        "# Load splits + quick helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "s6-kBI8_T9Gw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "SPLITS = Path(r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\splits\")\n",
        "\n",
        "train   = pd.read_parquet(SPLITS / \"train_indexed.parquet\")         # [uid, iid, ts]\n",
        "val     = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\")   # [userId, uid, val_item, iid, ts_val]\n",
        "test    = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")  # [userId, uid, test_item, iid, ts_test]\n",
        "\n",
        "\n",
        "U = int(train[\"uid\"].max()) + 1\n",
        "I = int(train[\"iid\"].max()) + 1\n",
        "\n",
        "# users×items implicit matrix\n",
        "R = csr_matrix((np.ones(len(train), dtype=np.float32),\n",
        "                (train[\"uid\"].astype(int), train[\"iid\"].astype(int))),\n",
        "               shape=(U, I))\n",
        "\n",
        "# fast lookup: items seen in TRAIN per user\n",
        "user_seen = train.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n",
        "\n",
        "# evaluation helper: candidate coverage (how often held-out item is in the pool)\n",
        "def candidate_coverage(cand_df, targets_df, tgt_col=\"iid\"):\n",
        "    # Join to bring the target iid next to the candidate list\n",
        "    df = cand_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\", how=\"inner\")\n",
        "    # Drop cold-start rows where iid is NaN (item not in TRAIN map)\n",
        "    df = df[df[tgt_col].notna()]\n",
        "    # Fast and robust: zip the target and the candidate list\n",
        "    return np.mean([int(t) in set(c) for t, c in zip(df[tgt_col], df[\"candidates\"])])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpEQftzUDUk"
      },
      "source": [
        "# Artifact A: Popularity (train-only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8691IfAeUFjl"
      },
      "outputs": [],
      "source": [
        "# TRAIN-ONLY popularity\n",
        "pop = (train.groupby(\"iid\").size()\n",
        "                 .sort_values(ascending=False))\n",
        "\n",
        "def top_pop_unseen(u_seen, P=50):\n",
        "    out = []\n",
        "    for iid in pop.index:\n",
        "        if iid not in u_seen:\n",
        "            out.append(int(iid))\n",
        "            if len(out) >= P:\n",
        "                break\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvHl_Km_UQKa"
      },
      "source": [
        "# Build candidate pools (popularity-only to start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrjASCypURPL",
        "outputId": "95900d54-9472-4c23-a416-bc5ae087221f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'val_idx' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m         rows\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mint\u001b[39m(u), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcandidates\u001b[39m\u001b[38;5;124m\"\u001b[39m: build_pool_for_user(\u001b[38;5;28mint\u001b[39m(u), P\u001b[38;5;241m=\u001b[39mP, K\u001b[38;5;241m=\u001b[39mK)})\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(rows)\n\u001b[1;32m---> 17\u001b[0m users_val  \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(val_idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique())  \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[43mval_idx\u001b[49m)  \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m     18\u001b[0m users_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(test_idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munique()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(test_idx) \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m     20\u001b[0m cand_val  \u001b[38;5;241m=\u001b[39m make_candidates(users_val,  P\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, K\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'val_idx' is not defined"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pyarrow as pa, pyarrow.parquet as pq\n",
        "\n",
        "def build_pool_for_user(uid, P=50, K=100):\n",
        "    seen = user_seen.get(uid, set())\n",
        "    C = top_pop_unseen(seen, P=P)\n",
        "    # de-dup + drop seen (pop already avoids seen, but keep it here for safety)\n",
        "    C = [iid for i, iid in enumerate(C) if iid not in seen and C.index(iid) == i]\n",
        "    return C[:K]\n",
        "\n",
        "def make_candidates(user_ids, P=50, K=100):\n",
        "    rows = []\n",
        "    for u in tqdm(user_ids):\n",
        "        rows.append({\"uid\": int(u), \"candidates\": build_pool_for_user(int(u), P=P, K=K)})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "users_val  = sorted(val_idx[\"uid\"].unique())  if len(val_idx)  else []\n",
        "users_test = sorted(test_idx[\"uid\"].unique()) if len(test_idx) else []\n",
        "\n",
        "cand_val  = make_candidates(users_val,  P=50, K=100)\n",
        "cand_test = make_candidates(users_test, P=50, K=100)\n",
        "\n",
        "# save\n",
        "OUT = SPLITS.parent / \"candidates\"\n",
        "OUT.mkdir(exist_ok=True)\n",
        "cand_val.to_parquet(OUT / \"val.parquet\", index=False)\n",
        "cand_test.to_parquet(OUT / \"test.parquet\", index=False)\n",
        "\n",
        "print(\"Wrote:\", OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl5crxFsUUBz"
      },
      "source": [
        "# Check candidate coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3qjvAGrUWOB",
        "outputId": "02cfb227-21a4-4345-fbd4-af4f45f3ef21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidate coverage  val=14.96%  test=13.80%\n"
          ]
        }
      ],
      "source": [
        "val_cov  = candidate_coverage(cand_val,  val_idx,  \"iid\") if len(cand_val)  else float(\"nan\")\n",
        "test_cov = candidate_coverage(cand_test, test_idx, \"iid\") if len(cand_test) else float(\"nan\")\n",
        "print(f\"Candidate coverage  val={val_cov:.2%}  test={test_cov:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMhHE0NBlH/BSXrGjJvquND",
      "include_colab_link": true,
      "mount_file_id": "1v14zduS8ZnCfMpMoMMIvkrRdQZqD7Ei0",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ece1508gp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
