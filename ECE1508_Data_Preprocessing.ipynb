{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epcdolO1Tlw5"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eithwTf8Tp6-"
      },
      "source": [
        "### Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wrote: C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\ratings.csv 1000209\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "RAW = Path(r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\")\n",
        "out_csv = RAW / \"ratings.csv\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    RAW / \"ratings.dat\",\n",
        "    sep=\"::\",\n",
        "    engine=\"python\",\n",
        "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
        ")\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(\"wrote:\", out_csv, len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "W_pCAnFf4jnu"
      },
      "outputs": [],
      "source": [
        "# Time-aware Leave-One-Out (LOO) split for MovieLens ratings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_ts_unit(ts_series):\n",
        "    vmax = float(ts_series.max())\n",
        "    return \"ms\" if vmax > 1e12 else \"s\"\n",
        "\n",
        "def time_aware_loo_split(\n",
        "    ratings_csv: str,\n",
        "    out_dir: str,\n",
        "    rating_threshold: float = 3.0,\n",
        "    min_positives: int = 3,\n",
        "    also_csv: bool = False,\n",
        "):\n",
        "    out = Path(out_dir); (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
        "    ratings = pd.read_csv(ratings_csv)\n",
        "\n",
        "    # 1) Basic checks\n",
        "    need = {\"userId\",\"movieId\",\"rating\",\"timestamp\"}\n",
        "    missing = need - set(ratings.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
        "\n",
        "    # 2) Normalize timestamp and keep implicit positives\n",
        "    unit = _detect_ts_unit(ratings[\"timestamp\"])\n",
        "    ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
        "\n",
        "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
        "    # Drop duplicate (user,item) -> keep earliest\n",
        "    pos = pos.sort_values([\"userId\",\"ts\",\"movieId\"], kind=\"mergesort\")\n",
        "    pos = pos.drop_duplicates([\"userId\",\"movieId\"], keep=\"first\")\n",
        "\n",
        "    # Keep users with at least min_positives\n",
        "    pos = pos[pos.groupby(\"userId\")[\"movieId\"].transform(\"size\") >= min_positives].copy()\n",
        "\n",
        "    # 3) Rank by time per user, assign splits: last=test, second last=val (if >=3), rest=train\n",
        "    pos[\"n\"] = pos.groupby(\"userId\")[\"movieId\"].transform(\"size\")\n",
        "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
        "    pos[\"split\"] = \"train\"\n",
        "    pos.loc[pos[\"idx\"] == pos[\"n\"]-1, \"split\"] = \"test\"\n",
        "    pos.loc[(pos[\"n\"]>=3) & (pos[\"idx\"] == pos[\"n\"]-2), \"split\"] = \"val\"\n",
        "\n",
        "    train = pos[pos[\"split\"]==\"train\"][[\"userId\",\"movieId\",\"ts\"]].reset_index(drop=True)\n",
        "    val_targets  = pos[pos[\"split\"]==\"val\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"val_item\",\"ts\":\"ts_val\"}).reset_index(drop=True)\n",
        "    test_targets = pos[pos[\"split\"]==\"test\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"test_item\",\"ts\":\"ts_test\"}).reset_index(drop=True)\n",
        "\n",
        "    # 4) Build ID maps from TRAIN only (contiguous 0..U-1 and 0..I-1)\n",
        "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"])\n",
        "    uids[\"uid\"] = range(len(uids))\n",
        "    iids = pd.DataFrame(sorted(train[\"movieId\"].unique()), columns=[\"movieId\"])\n",
        "    iids[\"iid\"] = range(len(iids))\n",
        "\n",
        "    # 5) Also provide indexed versions (useful for MF/ALS)\n",
        "    train_idx = (train.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                      .merge(iids, on=\"movieId\", how=\"inner\"))\n",
        "    val_idx = None\n",
        "    if len(val_targets):\n",
        "        val_idx = (val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                              .merge(iids, left_on=\"val_item\", right_on=\"movieId\", how=\"left\")\n",
        "                              .drop(columns=[\"movieId\"]))\n",
        "    test_idx = (test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                               .merge(iids, left_on=\"test_item\", right_on=\"movieId\", how=\"left\")\n",
        "                               .drop(columns=[\"movieId\"]))\n",
        "\n",
        "    # 6) Save outputs\n",
        "    sp = out / \"splits\"\n",
        "    train.to_parquet(sp / \"train.parquet\", index=False)\n",
        "    if len(val_targets):\n",
        "        val_targets.to_parquet(sp / \"val_targets.parquet\", index=False)\n",
        "    test_targets.to_parquet(sp / \"test_targets.parquet\", index=False)\n",
        "    uids.to_parquet(sp / \"user_id_map.parquet\", index=False)\n",
        "    iids.to_parquet(sp / \"item_id_map.parquet\", index=False)\n",
        "    train_idx.to_parquet(sp / \"train_indexed.parquet\", index=False)\n",
        "    if val_idx is not None:\n",
        "        val_idx.to_parquet(sp / \"val_targets_indexed.parquet\", index=False)\n",
        "    test_idx.to_parquet(sp / \"test_targets_indexed.parquet\", index=False)\n",
        "\n",
        "    if also_csv:\n",
        "        train.to_csv(sp / \"train.csv\", index=False)\n",
        "        if len(val_targets): val_targets.to_csv(sp / \"val_targets.csv\", index=False)\n",
        "        test_targets.to_csv(sp / \"test_targets.csv\", index=False)\n",
        "        uids.to_csv(sp / \"user_id_map.csv\", index=False)\n",
        "        iids.to_csv(sp / \"item_id_map.csv\", index=False)\n",
        "        train_idx.to_csv(sp / \"train_indexed.csv\", index=False)\n",
        "        if val_idx is not None:\n",
        "            val_idx.to_csv(sp / \"val_targets_indexed.csv\", index=False)\n",
        "        test_idx.to_csv(sp / \"test_targets_indexed.csv\", index=False)\n",
        "\n",
        "    # 7) Quick stats + cold-start counts\n",
        "    cold_val = int(val_idx[\"iid\"].isna().sum()) if val_idx is not None and \"iid\" in val_idx else 0\n",
        "    cold_test = int(test_idx[\"iid\"].isna().sum()) if \"iid\" in test_idx else 0\n",
        "    stats = f\"\"\"Time-aware LOO split summary\n",
        "Users (TRAIN map): {len(uids)}\n",
        "Items (TRAIN map): {len(iids)}\n",
        "TRAIN positives  : {len(train)}\n",
        "VAL users        : {len(val_targets[\"userId\"].unique()) if len(val_targets) else 0}\n",
        "TEST users       : {len(test_targets[\"userId\"].unique())}\n",
        "Cold-start VAL items  : {cold_val}\n",
        "Cold-start TEST items : {cold_test}\n",
        "\"\"\"\n",
        "    (sp / \"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
        "    print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThfMPG-5Txll"
      },
      "source": [
        "### Call Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boIFiP3OAAgz",
        "outputId": "94890907-82a2-42a9-8279-cff31decd1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time-aware LOO split summary\n",
            "Users (TRAIN map): 6038\n",
            "Items (TRAIN map): 3623\n",
            "TRAIN positives  : 824401\n",
            "VAL users        : 6038\n",
            "TEST users       : 6038\n",
            "Cold-start VAL items  : 0\n",
            "Cold-start TEST items : 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "time_aware_loo_split(\n",
        "    ratings_csv=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\ratings.csv\",\n",
        "    out_dir=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\",\n",
        "    rating_threshold=3.0,   # implicit “like” threshold\n",
        "    min_positives=3,        # drop users with <3 positives\n",
        "    also_csv=True           # optional CSV mirrors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-rj0cuAT4t8"
      },
      "source": [
        "# Load splits + quick helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "s6-kBI8_T9Gw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "users: 6038 items: 3623 train rows: 824401\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "SPLITS = Path(r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\splits\")\n",
        "\n",
        "train = pd.read_parquet(SPLITS / \"train_indexed.parquet\")         # [uid, iid, ts]\n",
        "val_idx = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\") # [userId, uid, val_item, iid, ts_val]\n",
        "test_idx = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")  # [userId, uid, test_item, iid, ts_test]\n",
        "\n",
        "# sizes\n",
        "U = int(train[\"uid\"].max()) + 1\n",
        "I = int(train[\"iid\"].max()) + 1\n",
        "print(\"users:\", U, \"items:\", I, \"train rows:\", len(train))\n",
        "\n",
        "# users×items implicit matrix (Windows-friendly dtypes)\n",
        "rows = train[\"uid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "cols = train[\"iid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "data = np.ones(len(train), dtype=np.float32)\n",
        "\n",
        "R = csr_matrix((data, (rows, cols)), shape=(U, I), dtype=np.float32)\n",
        "\n",
        "# fast lookup: items seen in TRAIN per user\n",
        "user_seen = train.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n",
        "\n",
        "# evaluation helper\n",
        "def candidate_coverage(cand_df, targets_df, tgt_col=\"iid\"):\n",
        "    df = cand_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\", how=\"inner\")\n",
        "    df = df[df[tgt_col].notna()]  # drop cold-start items\n",
        "    return np.mean([int(t) in set(c) for t, c in zip(df[tgt_col], df[\"candidates\"])])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpEQftzUDUk"
      },
      "source": [
        "# Artifact A: Popularity (train-only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8691IfAeUFjl"
      },
      "outputs": [],
      "source": [
        "# TRAIN-only popularity\n",
        "pop = (\n",
        "    train.groupby(\"iid\").size()\n",
        "         .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "def top_pop_unseen(u_seen, P=50):\n",
        "    out = []\n",
        "    for iid in pop.index:\n",
        "        if iid not in u_seen:\n",
        "            out.append(int(iid))\n",
        "            if len(out) >= P:\n",
        "                break\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artifact B: item-item (w sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn item–item fitted on (3623, 6038)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# item × user matrix\n",
        "item_users = R.T.tocsr()\n",
        "item_users.sort_indices()\n",
        "\n",
        "knn = NearestNeighbors(\n",
        "    n_neighbors=51,      # 50 real neighbors\n",
        "    metric=\"cosine\",\n",
        "    algorithm=\"brute\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "knn.fit(item_users)\n",
        "print(\"sklearn item–item fitted on\", item_users.shape)\n",
        "\n",
        "def item_neighbors_from_history_sklearn(u_seen, per_item=20):\n",
        "    C = []\n",
        "    for iid in u_seen:\n",
        "        dists, idxs = knn.kneighbors(item_users[iid], n_neighbors=per_item+1, return_distance=True)\n",
        "        neigh_ids = idxs[0][1:]   # drop self\n",
        "        C.extend(int(j) for j in neigh_ids)\n",
        "    return C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artifact C: ALS Train only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 18.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALS trained.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from implicit.als import AlternatingLeastSquares\n",
        "\n",
        "# ALS model (we can tune later)\n",
        "als = AlternatingLeastSquares(\n",
        "    factors=64,\n",
        "    regularization=0.05,\n",
        "    iterations=10,\n",
        "    use_gpu=False\n",
        ")\n",
        "\n",
        "# implicit ALS wants item×user\n",
        "als.fit(R.T.tocsr())\n",
        "\n",
        "print(\"ALS trained.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvHl_Km_UQKa"
      },
      "source": [
        "# Build candidate pools (popularity/item-item,als)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrjASCypURPL",
        "outputId": "95900d54-9472-4c23-a416-bc5ae087221f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6038/6038 [47:07<00:00,  2.14it/s]\n",
            "100%|██████████| 6038/6038 [48:23<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote candidates to: C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\candidates\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pyarrow as pa, pyarrow.parquet as pq\n",
        "\n",
        "def mf_top_unseen(uid: int, Rk):\n",
        "    # ALS recommend already filters liked items\n",
        "    ids, scores = als.recommend(\n",
        "        userid=int(uid),\n",
        "        user_items=R[int(uid)],\n",
        "        N=Rk,\n",
        "        filter_already_liked_items=True,\n",
        "        recalculate_user=True,\n",
        "    )\n",
        "    return [int(i) for i in ids]\n",
        "\n",
        "def build_pool_for_user(uid: int,\n",
        "                        P=200,        # how many pop to try\n",
        "                        K=400,       # final cap\n",
        "                        Rk=200,       # how many ALS recs to try\n",
        "                        knn_per_item=20):  # how many neighbors per seen item\n",
        "    seen = user_seen.get(uid, set())\n",
        "    seen_set = set(seen)\n",
        "\n",
        "    C = []\n",
        "\n",
        "    # 1) popularity (train-only)\n",
        "    C += top_pop_unseen(seen, P=P)\n",
        "\n",
        "    # 2) ALS / MF (only if user is warm)\n",
        "    if len(seen):\n",
        "        C += mf_top_unseen(uid, Rk=Rk)\n",
        "\n",
        "    # 3) item–item (sklearn) — only if user has history\n",
        "    if len(seen):\n",
        "        few = list(seen)[:12]          # only 5 items from history\n",
        "        C += item_neighbors_from_history_sklearn(few, per_item=knn_per_item)\n",
        "\n",
        "    # 4) de-dup + drop seen + cap\n",
        "    dedup = []\n",
        "    for iid in C:\n",
        "        if iid in seen_set:\n",
        "            continue\n",
        "        if iid in dedup:\n",
        "            continue\n",
        "        dedup.append(iid)\n",
        "        if len(dedup) >= K:\n",
        "            break\n",
        "\n",
        "    return dedup\n",
        "\n",
        "\n",
        "def make_candidates(user_ids, P=200, K=400, Rk=200, knn_per_item=20):\n",
        "    rows = []\n",
        "    for u in tqdm(user_ids):\n",
        "        rows.append({\n",
        "            \"uid\": int(u),\n",
        "            \"candidates\": build_pool_for_user(\n",
        "                int(u),\n",
        "                P=P,\n",
        "                K=K,\n",
        "                Rk=Rk,\n",
        "                knn_per_item=knn_per_item,\n",
        "            )\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# who to build for\n",
        "users_val  = sorted(val_idx[\"uid\"].unique())  if len(val_idx)  else []\n",
        "users_test = sorted(test_idx[\"uid\"].unique()) if len(test_idx) else []\n",
        "\n",
        "# build pools\n",
        "cand_val  = make_candidates(users_val,  P=220,  K=550, Rk=250, knn_per_item=20)\n",
        "cand_test = make_candidates(users_test, P=220,  K=550, Rk=250, knn_per_item=20)\n",
        "\n",
        "# save\n",
        "OUT = SPLITS.parent / \"candidates\"\n",
        "OUT.mkdir(exist_ok=True)\n",
        "cand_val.to_parquet(OUT / \"val.parquet\", index=False)\n",
        "cand_test.to_parquet(OUT / \"test.parquet\", index=False)\n",
        "print(\"Wrote candidates to:\", OUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl5crxFsUUBz"
      },
      "source": [
        "# Check candidate coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3qjvAGrUWOB",
        "outputId": "02cfb227-21a4-4345-fbd4-af4f45f3ef21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidate coverage  val=50.70%  test=47.17%\n"
          ]
        }
      ],
      "source": [
        "val_cov  = candidate_coverage(cand_val,  val_idx,  \"iid\") if len(cand_val)  else float(\"nan\")\n",
        "test_cov = candidate_coverage(cand_test, test_idx, \"iid\") if len(cand_test) else float(\"nan\")\n",
        "print(f\"Candidate coverage  val={val_cov:.2%}  test={test_cov:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load & Define Covered Users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_item: dropped 0 rows (cold-start/invalid); kept 6038\n",
            "test_item: dropped 5 rows (cold-start/invalid); kept 6033\n",
            "Coverage  val=50.70%  test=47.17%\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ROOT = Path(\"C:/Users/abdul/ece1508gp/movielens_dataset\")\n",
        "SPLITS = ROOT / \"splits\"\n",
        "CANDS  = ROOT / \"candidates\"\n",
        "\n",
        "train_idx = pd.read_parquet(SPLITS / \"train_indexed.parquet\")   # [uid,iid,ts]\n",
        "val_idx   = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\")\n",
        "test_idx  = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")\n",
        "\n",
        "cand_val  = pd.read_parquet(CANDS / \"val.parquet\")   # [uid, candidates(list[iid])]\n",
        "cand_test = pd.read_parquet(CANDS / \"test.parquet\")\n",
        "\n",
        "def coverage_rows(cands_df, targets_df, tgt_name):\n",
        "    # merge candidates with target iid (indexed) for the same uid\n",
        "    df = cands_df.merge(\n",
        "        targets_df[[\"uid\", \"iid\"]].rename(columns={\"iid\": tgt_name}),\n",
        "        on=\"uid\", how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # drop cold-start targets (iid is NaN) and rows without candidates\n",
        "    before = len(df)\n",
        "    df = df[df[tgt_name].notna() & df[\"candidates\"].notna()].copy()\n",
        "    dropped = before - len(df)\n",
        "\n",
        "    # ensure candidates are lists\n",
        "    df[\"candidates\"] = df[\"candidates\"].apply(lambda x: list(x) if isinstance(x, (list, tuple, np.ndarray)) else [])\n",
        "\n",
        "    # use pandas nullable Int64 to avoid int(NaN) issues\n",
        "    df[tgt_name] = df[tgt_name].astype(\"Int64\")\n",
        "\n",
        "    # membership test (no int(...) cast needed now)\n",
        "    df[\"target_in_pool\"] = [t in set(c) for t, c in zip(df[tgt_name], df[\"candidates\"])]\n",
        "\n",
        "    print(f\"{tgt_name}: dropped {dropped} rows (cold-start/invalid); kept {len(df)}\")\n",
        "    return df\n",
        "\n",
        "val_cov  = coverage_rows(cand_val,  val_idx,  \"val_item\")\n",
        "test_cov = coverage_rows(cand_test, test_idx, \"test_item\")\n",
        "\n",
        "val_covered  = val_cov[val_cov[\"target_in_pool\"]].copy()\n",
        "test_covered = test_cov[test_cov[\"target_in_pool\"]].copy()\n",
        "\n",
        "print(f\"Coverage  val={len(val_covered)/len(val_cov):.2%}  test={len(test_covered)/len(test_cov):.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Users valid for eval  val=3061  test=2846\n"
          ]
        }
      ],
      "source": [
        "def mark_coverage(cands_df: pd.DataFrame, targets_df: pd.DataFrame, tgt_name: str):\n",
        "    df = cands_df.merge(\n",
        "        targets_df[[\"uid\",\"iid\"]].rename(columns={\"iid\": tgt_name}),\n",
        "        on=\"uid\", how=\"inner\"\n",
        "    )\n",
        "    # guard against NaNs and non-lists\n",
        "    df[\"candidates\"] = df[\"candidates\"].apply(\n",
        "        lambda x: list(x) if isinstance(x, (list, tuple, np.ndarray, pd.Series)) else []\n",
        "    )\n",
        "    # fill NaNs to avoid int() on NaN\n",
        "    tgt = df[tgt_name].fillna(-1).astype(int).to_numpy()\n",
        "    df[\"target_in_pool\"] = [int(t) in set(c) for t, c in zip(tgt, df[\"candidates\"])]\n",
        "    return df[[\"uid\",\"target_in_pool\"]]\n",
        "\n",
        "val_mark  = mark_coverage(cand_val,  val_idx,  \"val_item\")\n",
        "test_mark = mark_coverage(cand_test, test_idx, \"test_item\")\n",
        "\n",
        "covered_val_uids  = set(val_mark.loc[val_mark[\"target_in_pool\"], \"uid\"])\n",
        "covered_test_uids = set(test_mark.loc[test_mark[\"target_in_pool\"], \"uid\"])\n",
        "\n",
        "print(f\"Users valid for eval  val={len(covered_val_uids)}  test={len(covered_test_uids)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval coverage val=100.00% test=100.00%\n"
          ]
        }
      ],
      "source": [
        "# Restrict targets and pools to the covered users only (these will have 100% coverage by construction)\n",
        "val_eval_targets  = val_idx[val_idx[\"uid\"].isin(covered_val_uids)].copy()\n",
        "test_eval_targets = test_idx[test_idx[\"uid\"].isin(covered_test_uids)].copy()\n",
        "\n",
        "cand_val_eval  = cand_val[cand_val[\"uid\"].isin(covered_val_uids)].reset_index(drop=True)\n",
        "cand_test_eval = cand_test[cand_test[\"uid\"].isin(covered_test_uids)].reset_index(drop=True)\n",
        "\n",
        "# (Optional) sanity check – should print 100%/100%\n",
        "def coverage_percent(cands_df, targets_df, tgt_col):\n",
        "    df = cands_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\")\n",
        "    return np.mean([int(t) in set(c) for t, c in zip(df[tgt_col], df[\"candidates\"])])\n",
        "\n",
        "print(\"Eval coverage\",\n",
        "      f\"val={coverage_percent(cand_val_eval,  val_eval_targets,  'iid'):.2%}\",\n",
        "      f\"test={coverage_percent(cand_test_eval, test_eval_targets, 'iid'):.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:01<00:00, 17.48it/s]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "\n",
        "# Build users×items CSR from train\n",
        "U = int(train_idx[\"uid\"].max()) + 1\n",
        "I = int(train_idx[\"iid\"].max()) + 1\n",
        "\n",
        "rows = train_idx[\"uid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "cols = train_idx[\"iid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "data = np.ones(len(train_idx), dtype=np.float32)\n",
        "\n",
        "R = csr_matrix((data, (rows, cols)), shape=(U, I), dtype=np.float32)\n",
        "\n",
        "# implicit expects item×user for fitting\n",
        "item_users = R.T.tocsr()\n",
        "\n",
        "als = AlternatingLeastSquares(\n",
        "    factors=64,          # try 64/96/128\n",
        "    regularization=0.02,\n",
        "    iterations=20,\n",
        "    dtype=np.float32,\n",
        ")\n",
        "als.fit(item_users)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 3628 is out of bounds for axis 0 with size 3623",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[52], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(hits)), \u001b[38;5;28mfloat\u001b[39m(np\u001b[38;5;241m.\u001b[39mmean(ndcgs))\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# VAL (use for tuning)\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m hr_v, ndcg_v \u001b[38;5;241m=\u001b[39m \u001b[43meval_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_als\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[VAL]  ALS HR@10=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhr_v\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  NDCG@10=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndcg_v\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# TEST (final report)\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[52], line 45\u001b[0m, in \u001b[0;36meval_pool\u001b[1;34m(df_eval, scorer, k)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cnds:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[43m(\u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[52], line 20\u001b[0m, in \u001b[0;36mscore_als\u001b[1;34m(uid, cand_iids)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cand_iids:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[1;32m---> 20\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43mU_f\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# keep only in-bounds item ids (defensive)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m c \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m cand_iids \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;241m<\u001b[39m I], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint64)\n",
            "\u001b[1;31mIndexError\u001b[0m: index 3628 is out of bounds for axis 0 with size 3623"
          ]
        }
      ],
      "source": [
        "# 0) Join pools with their (covered) targets -> one tidy frame per split\n",
        "val_eval  = (cand_val_eval\n",
        "             .merge(val_eval_targets[[\"uid\",\"iid\"]]\n",
        "                    .rename(columns={\"iid\":\"target\"}), on=\"uid\"))\n",
        "test_eval = (cand_test_eval\n",
        "             .merge(test_eval_targets[[\"uid\",\"iid\"]]\n",
        "                    .rename(columns={\"iid\":\"target\"}), on=\"uid\"))\n",
        "\n",
        "# 1) Cache ALS factors\n",
        "U_f = als.user_factors            # [U, F]\n",
        "V_f = als.item_factors            # [I, F]\n",
        "I   = V_f.shape[0]\n",
        "\n",
        "def score_als(uid: int, cand_iids: list[int]) -> np.ndarray:\n",
        "    \"\"\"Return ALS scores for the candidate items of a user.\"\"\"\n",
        "    if not cand_iids:\n",
        "        return np.asarray([])\n",
        "    u = U_f[int(uid)]\n",
        "    # keep only in-bounds item ids (defensive)\n",
        "    c = np.array([int(i) for i in cand_iids if 0 <= int(i) < I], dtype=np.int64)\n",
        "    if c.size == 0:\n",
        "        return np.asarray([])\n",
        "    return V_f[c] @ u\n",
        "\n",
        "def hit_at_k(ranked, tgt, k=10):\n",
        "    return 1.0 if tgt in ranked[:k] else 0.0\n",
        "\n",
        "def ndcg_at_k(ranked, tgt, k=10):\n",
        "    for rank, iid in enumerate(ranked[:k], start=1):\n",
        "        if iid == tgt:\n",
        "            return 1.0 / np.log2(rank + 1 + 1)  # +1 in denom because ranks start at 1\n",
        "    return 0.0\n",
        "\n",
        "def eval_pool(df_eval: pd.DataFrame, scorer, k=10):\n",
        "    \"\"\"df_eval must have columns: uid, candidates (list[int]), target (int).\"\"\"\n",
        "    hits, ndcgs = [], []\n",
        "    for _, r in df_eval.iterrows():\n",
        "        uid  = int(r[\"uid\"])\n",
        "        cnds = [int(x) for x in (r[\"candidates\"] if isinstance(r[\"candidates\"], (list, tuple, np.ndarray, pd.Series)) else [])]\n",
        "        tgt  = int(r[\"target\"])\n",
        "        if not cnds:\n",
        "            continue\n",
        "        scores = scorer(uid, cnds)\n",
        "        if scores.size == 0:\n",
        "            continue\n",
        "        order  = np.argsort(-scores)\n",
        "        ranked = [cnds[i] for i in order]\n",
        "        hits.append(hit_at_k(ranked, tgt, k))\n",
        "        ndcgs.append(ndcg_at_k(ranked, tgt, k))\n",
        "    return float(np.mean(hits)), float(np.mean(ndcgs))\n",
        "\n",
        "# VAL (use for tuning)\n",
        "hr_v, ndcg_v = eval_pool(val_eval, score_als, k=10)\n",
        "print(f\"[VAL]  ALS HR@10={hr_v:.3f}  NDCG@10={ndcg_v:.3f}\")\n",
        "\n",
        "# TEST (final report)\n",
        "hr_t, ndcg_t = eval_pool(test_eval, score_als, k=10)\n",
        "print(f\"[TEST] ALS HR@10={hr_t:.3f}  NDCG@10={ndcg_t:.3f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMhHE0NBlH/BSXrGjJvquND",
      "include_colab_link": true,
      "mount_file_id": "1v14zduS8ZnCfMpMoMMIvkrRdQZqD7Ei0",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ece1508gp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
