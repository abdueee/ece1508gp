{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epcdolO1Tlw5"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eithwTf8Tp6-"
      },
      "source": [
        "### Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W_pCAnFf4jnu"
      },
      "outputs": [],
      "source": [
        "# Time-aware Leave-One-Out (LOO) split for MovieLens ratings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_ts_unit(ts_series):\n",
        "    vmax = float(ts_series.max())\n",
        "    return \"ms\" if vmax > 1e12 else \"s\"\n",
        "\n",
        "def time_aware_loo_split(\n",
        "    ratings_csv: str,\n",
        "    out_dir: str,\n",
        "    rating_threshold: float = 4.0,\n",
        "    min_positives: int = 2,\n",
        "    also_csv: bool = False,\n",
        "):\n",
        "    out = Path(out_dir); (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
        "    ratings = pd.read_csv(ratings_csv)\n",
        "\n",
        "    # 1) Basic checks\n",
        "    need = {\"userId\",\"movieId\",\"rating\",\"timestamp\"}\n",
        "    missing = need - set(ratings.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
        "\n",
        "    # 2) Normalize timestamp and keep implicit positives\n",
        "    unit = _detect_ts_unit(ratings[\"timestamp\"])\n",
        "    ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
        "\n",
        "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
        "    # Drop duplicate (user,item) -> keep earliest\n",
        "    pos = pos.sort_values([\"userId\",\"ts\",\"movieId\"], kind=\"mergesort\")\n",
        "    pos = pos.drop_duplicates([\"userId\",\"movieId\"], keep=\"first\")\n",
        "\n",
        "    # Keep users with at least min_positives\n",
        "    pos = pos[pos.groupby(\"userId\")[\"movieId\"].transform(\"size\") >= min_positives].copy()\n",
        "\n",
        "    # 3) Rank by time per user, assign splits: last=test, second last=val (if >=3), rest=train\n",
        "    pos[\"n\"] = pos.groupby(\"userId\")[\"movieId\"].transform(\"size\")\n",
        "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
        "    pos[\"split\"] = \"train\"\n",
        "    pos.loc[pos[\"idx\"] == pos[\"n\"]-1, \"split\"] = \"test\"\n",
        "    pos.loc[(pos[\"n\"]>=3) & (pos[\"idx\"] == pos[\"n\"]-2), \"split\"] = \"val\"\n",
        "\n",
        "    train = pos[pos[\"split\"]==\"train\"][[\"userId\",\"movieId\",\"ts\"]].reset_index(drop=True)\n",
        "    val_targets  = pos[pos[\"split\"]==\"val\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"val_item\",\"ts\":\"ts_val\"}).reset_index(drop=True)\n",
        "    test_targets = pos[pos[\"split\"]==\"test\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"test_item\",\"ts\":\"ts_test\"}).reset_index(drop=True)\n",
        "\n",
        "    # 4) Build ID maps from TRAIN only (contiguous 0..U-1 and 0..I-1)\n",
        "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"])\n",
        "    uids[\"uid\"] = range(len(uids))\n",
        "    iids = pd.DataFrame(sorted(train[\"movieId\"].unique()), columns=[\"movieId\"])\n",
        "    iids[\"iid\"] = range(len(iids))\n",
        "\n",
        "    # 5) Also provide indexed versions (useful for MF/ALS)\n",
        "    train_idx = (train.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                      .merge(iids, on=\"movieId\", how=\"inner\"))\n",
        "    val_idx = None\n",
        "    if len(val_targets):\n",
        "        val_idx = (val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                              .merge(iids, left_on=\"val_item\", right_on=\"movieId\", how=\"left\")\n",
        "                              .drop(columns=[\"movieId\"]))\n",
        "    test_idx = (test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                               .merge(iids, left_on=\"test_item\", right_on=\"movieId\", how=\"left\")\n",
        "                               .drop(columns=[\"movieId\"]))\n",
        "\n",
        "    # 6) Save outputs\n",
        "    sp = out / \"splits\"\n",
        "    train.to_parquet(sp / \"train.parquet\", index=False)\n",
        "    if len(val_targets):\n",
        "        val_targets.to_parquet(sp / \"val_targets.parquet\", index=False)\n",
        "    test_targets.to_parquet(sp / \"test_targets.parquet\", index=False)\n",
        "    uids.to_parquet(sp / \"user_id_map.parquet\", index=False)\n",
        "    iids.to_parquet(sp / \"item_id_map.parquet\", index=False)\n",
        "    train_idx.to_parquet(sp / \"train_indexed.parquet\", index=False)\n",
        "    if val_idx is not None:\n",
        "        val_idx.to_parquet(sp / \"val_targets_indexed.parquet\", index=False)\n",
        "    test_idx.to_parquet(sp / \"test_targets_indexed.parquet\", index=False)\n",
        "\n",
        "    if also_csv:\n",
        "        train.to_csv(sp / \"train.csv\", index=False)\n",
        "        if len(val_targets): val_targets.to_csv(sp / \"val_targets.csv\", index=False)\n",
        "        test_targets.to_csv(sp / \"test_targets.csv\", index=False)\n",
        "        uids.to_csv(sp / \"user_id_map.csv\", index=False)\n",
        "        iids.to_csv(sp / \"item_id_map.csv\", index=False)\n",
        "        train_idx.to_csv(sp / \"train_indexed.csv\", index=False)\n",
        "        if val_idx is not None:\n",
        "            val_idx.to_csv(sp / \"val_targets_indexed.csv\", index=False)\n",
        "        test_idx.to_csv(sp / \"test_targets_indexed.csv\", index=False)\n",
        "\n",
        "    # 7) Quick stats + cold-start counts\n",
        "    cold_val = int(val_idx[\"iid\"].isna().sum()) if val_idx is not None and \"iid\" in val_idx else 0\n",
        "    cold_test = int(test_idx[\"iid\"].isna().sum()) if \"iid\" in test_idx else 0\n",
        "    stats = f\"\"\"Time-aware LOO split summary\n",
        "Users (TRAIN map): {len(uids)}\n",
        "Items (TRAIN map): {len(iids)}\n",
        "TRAIN positives  : {len(train)}\n",
        "VAL users        : {len(val_targets[\"userId\"].unique()) if len(val_targets) else 0}\n",
        "TEST users       : {len(test_targets[\"userId\"].unique())}\n",
        "Cold-start VAL items  : {cold_val}\n",
        "Cold-start TEST items : {cold_test}\n",
        "\"\"\"\n",
        "    (sp / \"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
        "    print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThfMPG-5Txll"
      },
      "source": [
        "### Call Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boIFiP3OAAgz",
        "outputId": "94890907-82a2-42a9-8279-cff31decd1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time-aware LOO split summary\n",
            "Users (TRAIN map): 200466\n",
            "Items (TRAIN map): 54711\n",
            "TRAIN positives  : 15537362\n",
            "VAL users        : 200143\n",
            "TEST users       : 200466\n",
            "Cold-start VAL items  : 221\n",
            "Cold-start TEST items : 257\n",
            "\n"
          ]
        }
      ],
      "source": [
        "time_aware_loo_split(\n",
        "    ratings_csv=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\ratings.csv\",\n",
        "    out_dir=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\",\n",
        "    rating_threshold=4.0,   # implicit “like” threshold\n",
        "    min_positives=2,        # drop users with <2 positives\n",
        "    also_csv=True           # optional CSV mirrors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-rj0cuAT4t8"
      },
      "source": [
        "# Load splits + quick helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "s6-kBI8_T9Gw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "users: 200466 items: 54711 train rows: 15537362\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "SPLITS = Path(r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\splits\")\n",
        "\n",
        "train = pd.read_parquet(SPLITS / \"train_indexed.parquet\")         # [uid, iid, ts]\n",
        "val_idx = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\") # [userId, uid, val_item, iid, ts_val]\n",
        "test_idx = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")  # [userId, uid, test_item, iid, ts_test]\n",
        "\n",
        "# sizes\n",
        "U = int(train[\"uid\"].max()) + 1\n",
        "I = int(train[\"iid\"].max()) + 1\n",
        "print(\"users:\", U, \"items:\", I, \"train rows:\", len(train))\n",
        "\n",
        "# users×items implicit matrix (Windows-friendly dtypes)\n",
        "rows = train[\"uid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "cols = train[\"iid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "data = np.ones(len(train), dtype=np.float32)\n",
        "\n",
        "R = csr_matrix((data, (rows, cols)), shape=(U, I), dtype=np.float32)\n",
        "\n",
        "# fast lookup: items seen in TRAIN per user\n",
        "user_seen = train.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n",
        "\n",
        "# evaluation helper\n",
        "def candidate_coverage(cand_df, targets_df, tgt_col=\"iid\"):\n",
        "    df = cand_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\", how=\"inner\")\n",
        "    df = df[df[tgt_col].notna()]  # drop cold-start items\n",
        "    return np.mean([int(t) in set(c) for t, c in zip(df[tgt_col], df[\"candidates\"])])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpEQftzUDUk"
      },
      "source": [
        "# Artifact A: Popularity (train-only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "8691IfAeUFjl"
      },
      "outputs": [],
      "source": [
        "# TRAIN-only popularity\n",
        "pop = (\n",
        "    train.groupby(\"iid\").size()\n",
        "         .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "def top_pop_unseen(u_seen, P=50):\n",
        "    out = []\n",
        "    for iid in pop.index:\n",
        "        if iid not in u_seen:\n",
        "            out.append(int(iid))\n",
        "            if len(out) >= P:\n",
        "                break\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artifact B: ALS Train only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
            "  check_blas_config()\n",
            "100%|██████████| 10/10 [00:20<00:00,  2.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALS trained.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from implicit.als import AlternatingLeastSquares\n",
        "\n",
        "# ALS model (we can tune later)\n",
        "als = AlternatingLeastSquares(\n",
        "    factors=64,\n",
        "    regularization=0.05,\n",
        "    iterations=10,\n",
        "    use_gpu=False\n",
        ")\n",
        "\n",
        "# implicit ALS wants item×user\n",
        "als.fit(R.T.tocsr())\n",
        "\n",
        "print(\"ALS trained.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvHl_Km_UQKa"
      },
      "source": [
        "# Build candidate pools (popularity-only to start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrjASCypURPL",
        "outputId": "95900d54-9472-4c23-a416-bc5ae087221f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 200143/200143 [18:12<00:00, 183.15it/s]\n",
            "100%|██████████| 200466/200466 [19:40<00:00, 169.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote candidates to: C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\candidates\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pyarrow as pa, pyarrow.parquet as pq\n",
        "\n",
        "def mf_top_unseen(uid: int, Rk=80):\n",
        "    # ALS recommend already filters liked items (we still have R for this user)\n",
        "    ids, scores = als.recommend(\n",
        "        userid=int(uid),\n",
        "        user_items=R[int(uid)],\n",
        "        N=Rk,\n",
        "        filter_already_liked_items=True,\n",
        "        recalculate_user=True,\n",
        "    )\n",
        "    return [int(i) for i in ids]\n",
        "\n",
        "def build_pool_for_user(uid: int, P=50, K=100, Rk=80):\n",
        "    seen = user_seen.get(uid, set())\n",
        "\n",
        "    # 1) pop (train-only)\n",
        "    C = top_pop_unseen(seen, P=P)\n",
        "\n",
        "    # 2) MF / ALS\n",
        "    C += mf_top_unseen(uid, Rk=Rk)\n",
        "\n",
        "    # 3) de-dup + drop seen + cap at K\n",
        "    dedup = []\n",
        "    seen_set = set(seen)\n",
        "    for x in C:\n",
        "        if x not in seen_set and x not in dedup:\n",
        "            dedup.append(x)\n",
        "            if len(dedup) >= K:\n",
        "                break\n",
        "\n",
        "    return dedup\n",
        "\n",
        "def make_candidates(user_ids, P=50, K=100, Rk=80):\n",
        "    rows = []\n",
        "    for u in tqdm(user_ids):\n",
        "        rows.append({\n",
        "            \"uid\": int(u),\n",
        "            \"candidates\": build_pool_for_user(int(u), P=P, K=K, Rk=Rk)\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# who to build for\n",
        "users_val  = sorted(val_idx[\"uid\"].unique())  if len(val_idx)  else []\n",
        "users_test = sorted(test_idx[\"uid\"].unique()) if len(test_idx) else []\n",
        "\n",
        "# build\n",
        "cand_val  = make_candidates(users_val,  P=120, K=300, Rk=80)\n",
        "cand_test = make_candidates(users_test, P=120, K=300, Rk=80)\n",
        "\n",
        "# save (same place as before)\n",
        "OUT = SPLITS.parent / \"candidates\"\n",
        "OUT.mkdir(exist_ok=True)\n",
        "cand_val.to_parquet(OUT / \"val.parquet\", index=False)\n",
        "cand_test.to_parquet(OUT / \"test.parquet\", index=False)\n",
        "\n",
        "print(\"Wrote candidates to:\", OUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl5crxFsUUBz"
      },
      "source": [
        "# Check candidate coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3qjvAGrUWOB",
        "outputId": "02cfb227-21a4-4345-fbd4-af4f45f3ef21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidate coverage  val=25.24%  test=23.63%\n"
          ]
        }
      ],
      "source": [
        "val_cov  = candidate_coverage(cand_val,  val_idx,  \"iid\") if len(cand_val)  else float(\"nan\")\n",
        "test_cov = candidate_coverage(cand_test, test_idx, \"iid\") if len(cand_test) else float(\"nan\")\n",
        "print(f\"Candidate coverage  val={val_cov:.2%}  test={test_cov:.2%}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMhHE0NBlH/BSXrGjJvquND",
      "include_colab_link": true,
      "mount_file_id": "1v14zduS8ZnCfMpMoMMIvkrRdQZqD7Ei0",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ece1508gp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
