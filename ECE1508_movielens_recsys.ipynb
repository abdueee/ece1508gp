{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epcdolO1Tlw5"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eithwTf8Tp6-"
      },
      "source": [
        "### Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      4\u001b[0m RAW \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mabdul\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mece1508gp\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmovielens_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\pandas\\__init__.py:61\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# let init-time option registration happen\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig_init\u001b[39;00m  \u001b[38;5;66;03m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# dtype\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     ArrowDtype,\n\u001b[0;32m     64\u001b[0m     Int8Dtype,\n\u001b[0;32m     65\u001b[0m     Int16Dtype,\n\u001b[0;32m     66\u001b[0m     Int32Dtype,\n\u001b[0;32m     67\u001b[0m     Int64Dtype,\n\u001b[0;32m     68\u001b[0m     UInt8Dtype,\n\u001b[0;32m     69\u001b[0m     UInt16Dtype,\n\u001b[0;32m     70\u001b[0m     UInt32Dtype,\n\u001b[0;32m     71\u001b[0m     UInt64Dtype,\n\u001b[0;32m     72\u001b[0m     Float32Dtype,\n\u001b[0;32m     73\u001b[0m     Float64Dtype,\n\u001b[0;32m     74\u001b[0m     CategoricalDtype,\n\u001b[0;32m     75\u001b[0m     PeriodDtype,\n\u001b[0;32m     76\u001b[0m     IntervalDtype,\n\u001b[0;32m     77\u001b[0m     DatetimeTZDtype,\n\u001b[0;32m     78\u001b[0m     StringDtype,\n\u001b[0;32m     79\u001b[0m     BooleanDtype,\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# missing\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     NA,\n\u001b[0;32m     82\u001b[0m     isna,\n\u001b[0;32m     83\u001b[0m     isnull,\n\u001b[0;32m     84\u001b[0m     notna,\n\u001b[0;32m     85\u001b[0m     notnull,\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;66;03m# indexes\u001b[39;00m\n\u001b[0;32m     87\u001b[0m     Index,\n\u001b[0;32m     88\u001b[0m     CategoricalIndex,\n\u001b[0;32m     89\u001b[0m     RangeIndex,\n\u001b[0;32m     90\u001b[0m     MultiIndex,\n\u001b[0;32m     91\u001b[0m     IntervalIndex,\n\u001b[0;32m     92\u001b[0m     TimedeltaIndex,\n\u001b[0;32m     93\u001b[0m     DatetimeIndex,\n\u001b[0;32m     94\u001b[0m     PeriodIndex,\n\u001b[0;32m     95\u001b[0m     IndexSlice,\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# tseries\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     NaT,\n\u001b[0;32m     98\u001b[0m     Period,\n\u001b[0;32m     99\u001b[0m     period_range,\n\u001b[0;32m    100\u001b[0m     Timedelta,\n\u001b[0;32m    101\u001b[0m     timedelta_range,\n\u001b[0;32m    102\u001b[0m     Timestamp,\n\u001b[0;32m    103\u001b[0m     date_range,\n\u001b[0;32m    104\u001b[0m     bdate_range,\n\u001b[0;32m    105\u001b[0m     Interval,\n\u001b[0;32m    106\u001b[0m     interval_range,\n\u001b[0;32m    107\u001b[0m     DateOffset,\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# conversion\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     to_numeric,\n\u001b[0;32m    110\u001b[0m     to_datetime,\n\u001b[0;32m    111\u001b[0m     to_timedelta,\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# misc\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     Flags,\n\u001b[0;32m    114\u001b[0m     Grouper,\n\u001b[0;32m    115\u001b[0m     factorize,\n\u001b[0;32m    116\u001b[0m     unique,\n\u001b[0;32m    117\u001b[0m     value_counts,\n\u001b[0;32m    118\u001b[0m     NamedAgg,\n\u001b[0;32m    119\u001b[0m     array,\n\u001b[0;32m    120\u001b[0m     Categorical,\n\u001b[0;32m    121\u001b[0m     set_eng_float_format,\n\u001b[0;32m    122\u001b[0m     Series,\n\u001b[0;32m    123\u001b[0m     DataFrame,\n\u001b[0;32m    124\u001b[0m )\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtseries\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m infer_freq\n",
            "File \u001b[1;32mc:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\pandas\\core\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     NaT,\n\u001b[0;32m      3\u001b[0m     Period,\n\u001b[0;32m      4\u001b[0m     Timedelta,\n\u001b[0;32m      5\u001b[0m     Timestamp,\n\u001b[0;32m      6\u001b[0m )\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmissing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NA\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ArrowDtype,\n\u001b[0;32m     11\u001b[0m     CategoricalDtype,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     PeriodDtype,\n\u001b[0;32m     15\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\pandas\\_libs\\__init__.py:17\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Below imports needs to happen first to ensure pandas top level\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# module gets monkeypatched with the pandas_datetime_CAPI\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# see pandas_datetime_exec in pd_datetime.c\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_parser\u001b[39;00m  \u001b[38;5;66;03m# isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas_datetime\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401 # isort: skip # type: ignore[reportUnusedImport]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minterval\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Interval\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtslibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     NaT,\n\u001b[0;32m     21\u001b[0m     NaTType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     iNaT,\n\u001b[0;32m     27\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "RAW = Path(r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\")\n",
        "out_csv = RAW / \"ratings.csv\"\n",
        "\n",
        "df = pd.read_csv(\n",
        "    RAW / \"ratings.dat\",\n",
        "    sep=\"::\",\n",
        "    engine=\"python\",\n",
        "    names=[\"userId\", \"movieId\", \"rating\", \"timestamp\"]\n",
        ")\n",
        "df.to_csv(out_csv, index=False)\n",
        "print(\"wrote:\", out_csv, len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "W_pCAnFf4jnu"
      },
      "outputs": [],
      "source": [
        "# Time-aware Leave-One-Out (LOO) split for MovieLens ratings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "def _detect_ts_unit(ts_series):\n",
        "    vmax = float(ts_series.max())\n",
        "    return \"ms\" if vmax > 1e12 else \"s\"\n",
        "\n",
        "def time_aware_loo_split(\n",
        "    ratings_csv: str,\n",
        "    out_dir: str,\n",
        "    rating_threshold: float = 3.0,\n",
        "    min_positives: int = 3,\n",
        "    also_csv: bool = False,\n",
        "):\n",
        "    out = Path(out_dir); (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
        "    ratings = pd.read_csv(ratings_csv)\n",
        "\n",
        "    # 1) Basic checks\n",
        "    need = {\"userId\",\"movieId\",\"rating\",\"timestamp\"}\n",
        "    missing = need - set(ratings.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
        "\n",
        "    # 2) Normalize timestamp and keep implicit positives\n",
        "    unit = _detect_ts_unit(ratings[\"timestamp\"])\n",
        "    ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
        "\n",
        "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
        "    # Drop duplicate (user,item) -> keep earliest\n",
        "    pos = pos.sort_values([\"userId\",\"ts\",\"movieId\"], kind=\"mergesort\")\n",
        "    pos = pos.drop_duplicates([\"userId\",\"movieId\"], keep=\"first\")\n",
        "\n",
        "    # Keep users with at least min_positives\n",
        "    pos = pos[pos.groupby(\"userId\")[\"movieId\"].transform(\"size\") >= min_positives].copy()\n",
        "\n",
        "    # 3) Rank by time per user, assign splits: last=test, second last=val (if >=3), rest=train\n",
        "    pos[\"n\"] = pos.groupby(\"userId\")[\"movieId\"].transform(\"size\")\n",
        "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
        "    pos[\"split\"] = \"train\"\n",
        "    pos.loc[pos[\"idx\"] == pos[\"n\"]-1, \"split\"] = \"test\"\n",
        "    pos.loc[(pos[\"n\"]>=3) & (pos[\"idx\"] == pos[\"n\"]-2), \"split\"] = \"val\"\n",
        "\n",
        "    train = pos[pos[\"split\"]==\"train\"][[\"userId\",\"movieId\",\"ts\"]].reset_index(drop=True)\n",
        "    val_targets  = pos[pos[\"split\"]==\"val\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"val_item\",\"ts\":\"ts_val\"}).reset_index(drop=True)\n",
        "    test_targets = pos[pos[\"split\"]==\"test\"][[\"userId\",\"movieId\",\"ts\"]].rename(\n",
        "        columns={\"movieId\":\"test_item\",\"ts\":\"ts_test\"}).reset_index(drop=True)\n",
        "\n",
        "    # 4) Build ID maps from TRAIN only (contiguous 0..U-1 and 0..I-1)\n",
        "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"])\n",
        "    uids[\"uid\"] = range(len(uids))\n",
        "    iids = pd.DataFrame(sorted(train[\"movieId\"].unique()), columns=[\"movieId\"])\n",
        "    iids[\"iid\"] = range(len(iids))\n",
        "\n",
        "    # 5) Also provide indexed versions (useful for MF/ALS)\n",
        "    train_idx = (train.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                      .merge(iids, on=\"movieId\", how=\"inner\"))\n",
        "    val_idx = None\n",
        "    if len(val_targets):\n",
        "        val_idx = (val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                              .merge(iids, left_on=\"val_item\", right_on=\"movieId\", how=\"left\")\n",
        "                              .drop(columns=[\"movieId\"]))\n",
        "    test_idx = (test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
        "                               .merge(iids, left_on=\"test_item\", right_on=\"movieId\", how=\"left\")\n",
        "                               .drop(columns=[\"movieId\"]))\n",
        "\n",
        "    # 6) Save outputs\n",
        "    sp = out / \"splits\"\n",
        "    train.to_parquet(sp / \"train.parquet\", index=False)\n",
        "    if len(val_targets):\n",
        "        val_targets.to_parquet(sp / \"val_targets.parquet\", index=False)\n",
        "    test_targets.to_parquet(sp / \"test_targets.parquet\", index=False)\n",
        "    uids.to_parquet(sp / \"user_id_map.parquet\", index=False)\n",
        "    iids.to_parquet(sp / \"item_id_map.parquet\", index=False)\n",
        "    train_idx.to_parquet(sp / \"train_indexed.parquet\", index=False)\n",
        "    if val_idx is not None:\n",
        "        val_idx.to_parquet(sp / \"val_targets_indexed.parquet\", index=False)\n",
        "    test_idx.to_parquet(sp / \"test_targets_indexed.parquet\", index=False)\n",
        "\n",
        "    if also_csv:\n",
        "        train.to_csv(sp / \"train.csv\", index=False)\n",
        "        if len(val_targets): val_targets.to_csv(sp / \"val_targets.csv\", index=False)\n",
        "        test_targets.to_csv(sp / \"test_targets.csv\", index=False)\n",
        "        uids.to_csv(sp / \"user_id_map.csv\", index=False)\n",
        "        iids.to_csv(sp / \"item_id_map.csv\", index=False)\n",
        "        train_idx.to_csv(sp / \"train_indexed.csv\", index=False)\n",
        "        if val_idx is not None:\n",
        "            val_idx.to_csv(sp / \"val_targets_indexed.csv\", index=False)\n",
        "        test_idx.to_csv(sp / \"test_targets_indexed.csv\", index=False)\n",
        "\n",
        "    # 7) Quick stats + cold-start counts\n",
        "    cold_val = int(val_idx[\"iid\"].isna().sum()) if val_idx is not None and \"iid\" in val_idx else 0\n",
        "    cold_test = int(test_idx[\"iid\"].isna().sum()) if \"iid\" in test_idx else 0\n",
        "    stats = f\"\"\"Time-aware LOO split summary\n",
        "Users (TRAIN map): {len(uids)}\n",
        "Items (TRAIN map): {len(iids)}\n",
        "TRAIN positives  : {len(train)}\n",
        "VAL users        : {len(val_targets[\"userId\"].unique()) if len(val_targets) else 0}\n",
        "TEST users       : {len(test_targets[\"userId\"].unique())}\n",
        "Cold-start VAL items  : {cold_val}\n",
        "Cold-start TEST items : {cold_test}\n",
        "\"\"\"\n",
        "    (sp / \"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
        "    print(stats)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThfMPG-5Txll"
      },
      "source": [
        "### Call Time Aware LOO Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boIFiP3OAAgz",
        "outputId": "94890907-82a2-42a9-8279-cff31decd1e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time-aware LOO split summary\n",
            "Users (TRAIN map): 6038\n",
            "Items (TRAIN map): 3623\n",
            "TRAIN positives  : 824401\n",
            "VAL users        : 6038\n",
            "TEST users       : 6038\n",
            "Cold-start VAL items  : 0\n",
            "Cold-start TEST items : 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "time_aware_loo_split(\n",
        "    ratings_csv=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\ratings.csv\",\n",
        "    out_dir=r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\",\n",
        "    rating_threshold=3.0,   # implicit “like” threshold\n",
        "    min_positives=3,        # drop users with <3 positives\n",
        "    also_csv=True           # optional CSV mirrors\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-rj0cuAT4t8"
      },
      "source": [
        "# Load splits + quick helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "s6-kBI8_T9Gw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "users: 6038 items: 3623 train rows: 824401\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "SPLITS = Path(r\"C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\splits\")\n",
        "\n",
        "train = pd.read_parquet(SPLITS / \"train_indexed.parquet\")         # [uid, iid, ts]\n",
        "val_idx = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\") # [userId, uid, val_item, iid, ts_val]\n",
        "test_idx = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")  # [userId, uid, test_item, iid, ts_test]\n",
        "\n",
        "# sizes\n",
        "U = int(train[\"uid\"].max()) + 1\n",
        "I = int(train[\"iid\"].max()) + 1\n",
        "print(\"users:\", U, \"items:\", I, \"train rows:\", len(train))\n",
        "\n",
        "# users×items implicit matrix (Windows-friendly dtypes)\n",
        "rows = train[\"uid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "cols = train[\"iid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "data = np.ones(len(train), dtype=np.float32)\n",
        "\n",
        "R = csr_matrix((data, (rows, cols)), shape=(U, I), dtype=np.float32)\n",
        "\n",
        "# fast lookup: items seen in TRAIN per user\n",
        "user_seen = train.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n",
        "\n",
        "# evaluation helper\n",
        "def candidate_coverage(cand_df, targets_df, tgt_col=\"iid\"):\n",
        "    df = cand_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\", how=\"inner\")\n",
        "    df = df[df[tgt_col].notna()]  # drop cold-start items\n",
        "    return np.mean([int(t) in set(c) for t, c in zip(df[tgt_col], df[\"candidates\"])])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMpEQftzUDUk"
      },
      "source": [
        "# Artifact A: Popularity (train-only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8691IfAeUFjl"
      },
      "outputs": [],
      "source": [
        "# TRAIN-only popularity\n",
        "pop = (\n",
        "    train.groupby(\"iid\").size()\n",
        "         .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "def top_pop_unseen(u_seen, P=50):\n",
        "    out = []\n",
        "    for iid in pop.index:\n",
        "        if iid not in u_seen:\n",
        "            out.append(int(iid))\n",
        "            if len(out) >= P:\n",
        "                break\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artifact B: item-item (w sklearn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sklearn item–item fitted on (3623, 6038)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "# item × user matrix\n",
        "item_users = R.T.tocsr()\n",
        "item_users.sort_indices()\n",
        "\n",
        "knn = NearestNeighbors(\n",
        "    n_neighbors=51,      # 50 real neighbors\n",
        "    metric=\"cosine\",\n",
        "    algorithm=\"brute\",\n",
        "    n_jobs=-1,\n",
        ")\n",
        "knn.fit(item_users)\n",
        "print(\"sklearn item–item fitted on\", item_users.shape)\n",
        "\n",
        "def item_neighbors_from_history_sklearn(u_seen, per_item=20):\n",
        "    C = []\n",
        "    for iid in u_seen:\n",
        "        dists, idxs = knn.kneighbors(item_users[iid], n_neighbors=per_item+1, return_distance=True)\n",
        "        neigh_ids = idxs[0][1:]   # drop self\n",
        "        C.extend(int(j) for j in neigh_ids)\n",
        "    return C\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Artifact C: ALS Train only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 18.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALS trained.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from implicit.als import AlternatingLeastSquares\n",
        "\n",
        "# ALS model (we can tune later)\n",
        "als = AlternatingLeastSquares(\n",
        "    factors=64,\n",
        "    regularization=0.05,\n",
        "    iterations=10,\n",
        "    use_gpu=False\n",
        ")\n",
        "\n",
        "# implicit ALS wants item×user\n",
        "als.fit(R.T.tocsr())\n",
        "\n",
        "print(\"ALS trained.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvHl_Km_UQKa"
      },
      "source": [
        "# Build candidate pools (w artifacts - popularity/item-item,als)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrjASCypURPL",
        "outputId": "95900d54-9472-4c23-a416-bc5ae087221f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 6038/6038 [47:07<00:00,  2.14it/s]\n",
            "100%|██████████| 6038/6038 [48:23<00:00,  2.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote candidates to: C:\\Users\\abdul\\ece1508gp\\movielens_dataset\\candidates\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import pyarrow as pa, pyarrow.parquet as pq\n",
        "\n",
        "def mf_top_unseen(uid: int, Rk):\n",
        "    # ALS recommend already filters liked items\n",
        "    ids, scores = als.recommend(\n",
        "        userid=int(uid),\n",
        "        user_items=R[int(uid)],\n",
        "        N=Rk,\n",
        "        filter_already_liked_items=True,\n",
        "        recalculate_user=True,\n",
        "    )\n",
        "    return [int(i) for i in ids]\n",
        "\n",
        "def build_pool_for_user(uid: int,\n",
        "                        P=200,        # how many pop to try\n",
        "                        K=400,       # final cap\n",
        "                        Rk=200,       # how many ALS recs to try\n",
        "                        knn_per_item=20):  # how many neighbors per seen item\n",
        "    seen = user_seen.get(uid, set())\n",
        "    seen_set = set(seen)\n",
        "\n",
        "    C = []\n",
        "\n",
        "    # 1) popularity (train-only)\n",
        "    C += top_pop_unseen(seen, P=P)\n",
        "\n",
        "    # 2) ALS / MF (only if user is warm)\n",
        "    if len(seen):\n",
        "        C += mf_top_unseen(uid, Rk=Rk)\n",
        "\n",
        "    # 3) item–item (sklearn) — only if user has history\n",
        "    if len(seen):\n",
        "        few = list(seen)[:12]          # only 5 items from history\n",
        "        C += item_neighbors_from_history_sklearn(few, per_item=knn_per_item)\n",
        "\n",
        "    # 4) de-dup + drop seen + cap\n",
        "    dedup = []\n",
        "    for iid in C:\n",
        "        if iid in seen_set:\n",
        "            continue\n",
        "        if iid in dedup:\n",
        "            continue\n",
        "        dedup.append(iid)\n",
        "        if len(dedup) >= K:\n",
        "            break\n",
        "\n",
        "    return dedup\n",
        "\n",
        "\n",
        "def make_candidates(user_ids, P=200, K=400, Rk=200, knn_per_item=20):\n",
        "    rows = []\n",
        "    for u in tqdm(user_ids):\n",
        "        rows.append({\n",
        "            \"uid\": int(u),\n",
        "            \"candidates\": build_pool_for_user(\n",
        "                int(u),\n",
        "                P=P,\n",
        "                K=K,\n",
        "                Rk=Rk,\n",
        "                knn_per_item=knn_per_item,\n",
        "            )\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "\n",
        "# who to build for\n",
        "users_val  = sorted(val_idx[\"uid\"].unique())  if len(val_idx)  else []\n",
        "users_test = sorted(test_idx[\"uid\"].unique()) if len(test_idx) else []\n",
        "\n",
        "# build pools\n",
        "cand_val  = make_candidates(users_val,  P=220,  K=550, Rk=250, knn_per_item=20)\n",
        "cand_test = make_candidates(users_test, P=220,  K=550, Rk=250, knn_per_item=20)\n",
        "\n",
        "# save\n",
        "OUT = SPLITS.parent / \"candidates\"\n",
        "OUT.mkdir(exist_ok=True)\n",
        "cand_val.to_parquet(OUT / \"val.parquet\", index=False)\n",
        "cand_test.to_parquet(OUT / \"test.parquet\", index=False)\n",
        "print(\"Wrote candidates to:\", OUT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl5crxFsUUBz"
      },
      "source": [
        "# Check candidate coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3qjvAGrUWOB",
        "outputId": "02cfb227-21a4-4345-fbd4-af4f45f3ef21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candidate coverage  val=50.70%  test=47.17%\n"
          ]
        }
      ],
      "source": [
        "val_cov  = candidate_coverage(cand_val,  val_idx,  \"iid\") if len(cand_val)  else float(\"nan\")\n",
        "test_cov = candidate_coverage(cand_test, test_idx, \"iid\") if len(cand_test) else float(\"nan\")\n",
        "print(f\"Candidate coverage  val={val_cov:.2%}  test={test_cov:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load & Define Covered Users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val_item: dropped 0 rows (cold-start/invalid); kept 6038\n",
            "test_item: dropped 5 rows (cold-start/invalid); kept 6033\n",
            "Coverage  val=50.70%  test=47.17%\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ROOT = Path(\"C:/Users/abdul/ece1508gp/movielens_dataset\")\n",
        "SPLITS = ROOT / \"splits\"\n",
        "CANDS  = ROOT / \"candidates\"\n",
        "\n",
        "train_idx = pd.read_parquet(SPLITS / \"train_indexed.parquet\")   # [uid,iid,ts]\n",
        "val_idx   = pd.read_parquet(SPLITS / \"val_targets_indexed.parquet\")\n",
        "test_idx  = pd.read_parquet(SPLITS / \"test_targets_indexed.parquet\")\n",
        "\n",
        "cand_val  = pd.read_parquet(CANDS / \"val.parquet\")   # [uid, candidates(list[iid])]\n",
        "cand_test = pd.read_parquet(CANDS / \"test.parquet\")\n",
        "\n",
        "def coverage_rows(cands_df, targets_df, tgt_name):\n",
        "    # merge candidates with target iid (indexed) for the same uid\n",
        "    df = cands_df.merge(\n",
        "        targets_df[[\"uid\", \"iid\"]].rename(columns={\"iid\": tgt_name}),\n",
        "        on=\"uid\", how=\"inner\"\n",
        "    )\n",
        "\n",
        "    # drop cold-start targets (iid is NaN) and rows without candidates\n",
        "    before = len(df)\n",
        "    df = df[df[tgt_name].notna() & df[\"candidates\"].notna()].copy()\n",
        "    dropped = before - len(df)\n",
        "\n",
        "    # ensure candidates are lists\n",
        "    df[\"candidates\"] = df[\"candidates\"].apply(lambda x: list(x) if isinstance(x, (list, tuple, np.ndarray)) else [])\n",
        "\n",
        "    # use pandas nullable Int64 to avoid int(NaN) issues\n",
        "    df[tgt_name] = df[tgt_name].astype(\"Int64\")\n",
        "\n",
        "    # membership test (no int(...) cast needed now)\n",
        "    df[\"target_in_pool\"] = [t in set(c) for t, c in zip(df[tgt_name], df[\"candidates\"])]\n",
        "\n",
        "    print(f\"{tgt_name}: dropped {dropped} rows (cold-start/invalid); kept {len(df)}\")\n",
        "    return df\n",
        "\n",
        "val_cov  = coverage_rows(cand_val,  val_idx,  \"val_item\")\n",
        "test_cov = coverage_rows(cand_test, test_idx, \"test_item\")\n",
        "\n",
        "val_covered  = val_cov[val_cov[\"target_in_pool\"]].copy()\n",
        "test_covered = test_cov[test_cov[\"target_in_pool\"]].copy()\n",
        "\n",
        "print(f\"Coverage  val={len(val_covered)/len(val_cov):.2%}  test={len(test_covered)/len(test_cov):.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Get Covered Users"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Users valid for eval  val=3061  test=2846\n"
          ]
        }
      ],
      "source": [
        "def mark_coverage(cands_df: pd.DataFrame, targets_df: pd.DataFrame, tgt_name: str):\n",
        "    df = cands_df.merge(\n",
        "        targets_df[[\"uid\",\"iid\"]].rename(columns={\"iid\": tgt_name}),\n",
        "        on=\"uid\", how=\"inner\"\n",
        "    )\n",
        "    # guard against NaNs and non-lists\n",
        "    df[\"candidates\"] = df[\"candidates\"].apply(\n",
        "        lambda x: list(x) if isinstance(x, (list, tuple, np.ndarray, pd.Series)) else []\n",
        "    )\n",
        "    # fill NaNs to avoid int() on NaN\n",
        "    tgt = df[tgt_name].fillna(-1).astype(int).to_numpy()\n",
        "    df[\"target_in_pool\"] = [int(t) in set(c) for t, c in zip(tgt, df[\"candidates\"])]\n",
        "    return df[[\"uid\",\"target_in_pool\"]]\n",
        "\n",
        "val_mark  = mark_coverage(cand_val,  val_idx,  \"val_item\")\n",
        "test_mark = mark_coverage(cand_test, test_idx, \"test_item\")\n",
        "\n",
        "covered_val_uids  = set(val_mark.loc[val_mark[\"target_in_pool\"], \"uid\"])\n",
        "covered_test_uids = set(test_mark.loc[test_mark[\"target_in_pool\"], \"uid\"])\n",
        "\n",
        "print(f\"Users valid for eval  val={len(covered_val_uids)}  test={len(covered_test_uids)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sanity check coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eval coverage val=100.00% test=100.00%\n"
          ]
        }
      ],
      "source": [
        "# Restrict targets and pools to the covered users only (these will have 100% coverage by construction)\n",
        "val_eval_targets  = val_idx[val_idx[\"uid\"].isin(covered_val_uids)].copy()\n",
        "test_eval_targets = test_idx[test_idx[\"uid\"].isin(covered_test_uids)].copy()\n",
        "\n",
        "cand_val_eval  = cand_val[cand_val[\"uid\"].isin(covered_val_uids)].reset_index(drop=True)\n",
        "cand_test_eval = cand_test[cand_test[\"uid\"].isin(covered_test_uids)].reset_index(drop=True)\n",
        "\n",
        "# (Optional) sanity check – should print 100%/100%\n",
        "def coverage_percent(cands_df, targets_df, tgt_col):\n",
        "    df = cands_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\")\n",
        "    return np.mean([int(t) in set(c) for t, c in zip(df[tgt_col], df[\"candidates\"])])\n",
        "\n",
        "print(\"Eval coverage\",\n",
        "      f\"val={coverage_percent(cand_val_eval,  val_eval_targets,  'iid'):.2%}\",\n",
        "      f\"test={coverage_percent(cand_test_eval, test_eval_targets, 'iid'):.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Train ALS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 12 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
            "  check_blas_config()\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.005249738693237305 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:16<00:00, 12.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ALS shapes  user_factors: (6038, 96)  item_factors: (3623, 96)  U: 6038  I: 3623\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.nearest_neighbours import bm25_weight, tfidf_weight \n",
        "\n",
        "# Build users × items CSR from train_idx \n",
        "U = int(train_idx[\"uid\"].max()) + 1\n",
        "I = int(train_idx[\"iid\"].max()) + 1\n",
        "\n",
        "rows = train_idx[\"uid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "cols = train_idx[\"iid\"].to_numpy(dtype=np.int32, copy=False)\n",
        "data = np.ones(len(train_idx), dtype=np.float32)\n",
        "\n",
        "alpha = 15\n",
        "R = csr_matrix((data, (rows, cols)), shape=(U, I), dtype=np.float32)\n",
        "X = bm25_weight(R, K1=100, B=0.8).astype(np.float32)\n",
        "\n",
        "# Train ALS on users × items (keeps names aligned) \n",
        "als = AlternatingLeastSquares(\n",
        "    factors=96,\n",
        "    regularization=0.05,\n",
        "    iterations=200,\n",
        "    use_gpu=False,     \n",
        "    dtype=np.float32,\n",
        ")\n",
        "als.fit(alpha*X)\n",
        "\n",
        "# Cache factors with consistent shapes\n",
        "U_f = als.user_factors   # shape [U, F]  user embeddings\n",
        "V_f = als.item_factors   # shape [I, F]  item embeddings\n",
        "\n",
        "print(\"ALS shapes  user_factors:\", U_f.shape, \" item_factors:\", V_f.shape, \" U:\", U, \" I:\", I)\n",
        "assert U_f.shape[0] == U and V_f.shape[0] == I\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  ALS HR@10=0.118  NDCG@10=0.055\n",
            "[TEST] ALS HR@10=0.110  NDCG@10=0.053\n"
          ]
        }
      ],
      "source": [
        "# 0) Join pools with their (covered) targets -> one tidy frame per split\n",
        "val_eval  = (cand_val_eval\n",
        "             .merge(val_eval_targets[[\"uid\",\"iid\"]]\n",
        "                    .rename(columns={\"iid\":\"target\"}), on=\"uid\"))\n",
        "test_eval = (cand_test_eval\n",
        "             .merge(test_eval_targets[[\"uid\",\"iid\"]]\n",
        "                    .rename(columns={\"iid\":\"target\"}), on=\"uid\"))\n",
        "\n",
        "# 1) Cache ALS factors\n",
        "U_f = als.user_factors            # [U, F]\n",
        "V_f = als.item_factors            # [I, F]\n",
        "I   = V_f.shape[0]\n",
        "\n",
        "def score_als(uid: int, cand_iids: list[int]) -> np.ndarray:\n",
        "    \"\"\"Return ALS scores for the candidate items of a user.\"\"\"\n",
        "    if not cand_iids:\n",
        "        return np.asarray([])\n",
        "    u = U_f[int(uid)]\n",
        "    # keep only in-bounds item ids (defensive)\n",
        "    c = np.array([int(i) for i in cand_iids if 0 <= int(i) < I], dtype=np.int64)\n",
        "    if c.size == 0:\n",
        "        return np.asarray([])\n",
        "    return V_f[c] @ u\n",
        "\n",
        "def hit_at_k(ranked, tgt, k=10):\n",
        "    return 1.0 if tgt in ranked[:k] else 0.0\n",
        "\n",
        "def ndcg_at_k(ranked, tgt, k=10):\n",
        "    for rank, iid in enumerate(ranked[:k], start=1):\n",
        "        if iid == tgt:\n",
        "            return 1.0 / np.log2(rank + 1)  # +1 in denom because ranks start at 1\n",
        "    return 0.0\n",
        "\n",
        "def eval_pool(df_eval: pd.DataFrame, scorer, k=10):\n",
        "    \"\"\"df_eval must have columns: uid, candidates (list[int]), target (int).\"\"\"\n",
        "    hits, ndcgs = [], []\n",
        "    for _, r in df_eval.iterrows():\n",
        "        uid  = int(r[\"uid\"])\n",
        "        cnds = [int(x) for x in (r[\"candidates\"] if isinstance(r[\"candidates\"], (list, tuple, np.ndarray, pd.Series)) else [])]\n",
        "        tgt  = int(r[\"target\"])\n",
        "        if not cnds:\n",
        "            continue\n",
        "        scores = scorer(uid, cnds)\n",
        "        if scores.size == 0:\n",
        "            continue\n",
        "        order  = np.argsort(-scores)\n",
        "        ranked = [cnds[i] for i in order]\n",
        "        hits.append(hit_at_k(ranked, tgt, k))\n",
        "        ndcgs.append(ndcg_at_k(ranked, tgt, k))\n",
        "    return float(np.mean(hits)), float(np.mean(ndcgs))\n",
        "\n",
        "# VAL (use for tuning)\n",
        "hr_v, ndcg_v = eval_pool(val_eval, score_als, k=10)\n",
        "print(f\"[VAL]  ALS HR@10={hr_v:.3f}  NDCG@10={ndcg_v:.3f}\")\n",
        "\n",
        "# TEST (final report)\n",
        "hr_t, ndcg_t = eval_pool(test_eval, score_als, k=10)\n",
        "print(f\"[TEST] ALS HR@10={hr_t:.3f}  NDCG@10={ndcg_t:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[VAL]  P@10=0.0118  R@10=0.1176  F1@10=0.0214\n",
            "[TEST] P@10=0.0110  R@10=0.1096  F1@10=0.0199\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def eval_precision_recall_f1(df_eval: pd.DataFrame, scorer, k=10):\n",
        "    \"\"\"df_eval: columns [uid, candidates(list[int]), target(int)]\"\"\"\n",
        "    precisions, recalls, f1s = [], [], []\n",
        "    for _, r in df_eval.iterrows():\n",
        "        uid  = int(r[\"uid\"])\n",
        "        cnds = r[\"candidates\"]\n",
        "        if not isinstance(cnds, (list, tuple, np.ndarray, pd.Series)) or len(cnds) == 0:\n",
        "            continue\n",
        "        cnds = [int(x) for x in cnds]\n",
        "\n",
        "        scores = scorer(uid, cnds)\n",
        "        if scores.size == 0:\n",
        "            continue\n",
        "\n",
        "        # get top-k efficiently and sort them\n",
        "        k_eff = min(k, scores.size)\n",
        "        topk_idx = np.argpartition(-scores, k_eff-1)[:k_eff]\n",
        "        topk_idx = topk_idx[np.argsort(-scores[topk_idx])]\n",
        "        topk_items = [cnds[i] for i in topk_idx]\n",
        "\n",
        "        hit = int(int(r[\"target\"]) in topk_items)\n",
        "        prec = hit / k                  # k is fixed per-user\n",
        "        rec  = float(hit)               # 1 relevant item in LOO\n",
        "        f1   = (2*prec*rec)/(prec+rec) if hit else 0.0\n",
        "\n",
        "        precisions.append(prec)\n",
        "        recalls.append(rec)\n",
        "        f1s.append(f1)\n",
        "\n",
        "    return float(np.mean(precisions)), float(np.mean(recalls)), float(np.mean(f1s))\n",
        "\n",
        "# Example usage with your ALS scorer:\n",
        "p_v, r_v, f1_v = eval_precision_recall_f1(val_eval,  score_als, k=10)\n",
        "p_t, r_t, f1_t = eval_precision_recall_f1(test_eval, score_als, k=10)\n",
        "print(f\"[VAL]  P@10={p_v:.4f}  R@10={r_v:.4f}  F1@10={f1_v:.4f}\")\n",
        "print(f\"[TEST] P@10={p_t:.4f}  R@10={r_t:.4f}  F1@10={f1_t:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.005292654037475586 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 150/150 [00:11<00:00, 13.38it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0041866302490234375 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 150/150 [00:11<00:00, 12.68it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.004279375076293945 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 150/150 [00:12<00:00, 12.23it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.005023479461669922 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:18<00:00, 11.06it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.004040241241455078 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:18<00:00, 10.93it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.005322933197021484 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:20<00:00,  9.84it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.004228830337524414 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:21<00:00,  9.24it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.00534510612487793 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:23<00:00,  8.59it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.005348920822143555 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:21<00:00,  9.34it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.005101919174194336 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:21<00:00,  9.31it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0052793025970458984 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:21<00:00,  9.17it/s]\n",
            "c:\\Users\\abdul\\.venvs\\ece1508gp\\lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.004019498825073242 seconds\n",
            "  warnings.warn(\n",
            "100%|██████████| 200/200 [00:21<00:00,  9.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   factors   reg  alpha  iters    val_HR  val_NDCG     val_P     val_R  \\\n",
            "2      128  0.10     10    200  0.123162  0.059147  0.012316  0.123162   \n",
            "3      128  0.05     20    200  0.119787  0.057406  0.011979  0.119787   \n",
            "1       96  0.05     15    200  0.119460  0.056065  0.011946  0.119460   \n",
            "0       64  0.05     15    150  0.107917  0.052059  0.010792  0.107917   \n",
            "\n",
            "     val_F1   test_HR  test_NDCG    test_P    test_R   test_F1  \n",
            "2  0.022393  0.117241   0.057561  0.011724  0.117241  0.021316  \n",
            "3  0.021779  0.110799   0.053867  0.011080  0.110799  0.020145  \n",
            "1  0.021720  0.108456   0.054178  0.010846  0.108456  0.019719  \n",
            "0  0.019621  0.099204   0.047572  0.009920  0.099204  0.018037  \n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from implicit.als import AlternatingLeastSquares\n",
        "from implicit.nearest_neighbours import bm25_weight\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Helper metric functions\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def hit_at_k(ranked, tgt, k=10):\n",
        "    return 1.0 if tgt in ranked[:k] else 0.0\n",
        "\n",
        "def ndcg_at_k(ranked, tgt, k=10):\n",
        "    for rank, iid in enumerate(ranked[:k], start=1):\n",
        "        if iid == tgt:\n",
        "            return 1.0 / np.log2(rank + 1)\n",
        "    return 0.0\n",
        "\n",
        "def precision_recall_f1(ranked, tgt, k=10):\n",
        "    hit = int(tgt in ranked[:k])\n",
        "    prec = hit / k\n",
        "    rec = float(hit)         # because only 1 relevant item in LOO\n",
        "    f1 = (2*prec*rec)/(prec+rec) if hit else 0.0\n",
        "    return prec, rec, f1\n",
        "\n",
        "def eval_pool(df_eval, scorer, k=10):\n",
        "    hits, ndcgs, precs, recs, f1s = [], [], [], [], []\n",
        "    for _, r in df_eval.iterrows():\n",
        "        uid  = int(r[\"uid\"])\n",
        "        cnds = r[\"candidates\"]\n",
        "        if not isinstance(cnds, (list, tuple, np.ndarray, pd.Series)) or len(cnds) == 0:\n",
        "            continue\n",
        "        cnds = [int(x) for x in cnds]\n",
        "        scores = scorer(uid, cnds)\n",
        "        if scores.size == 0:\n",
        "            continue\n",
        "        order  = np.argsort(-scores)\n",
        "        ranked = [cnds[i] for i in order]\n",
        "        hits.append(hit_at_k(ranked, int(r[\"target\"]), k))\n",
        "        ndcgs.append(ndcg_at_k(ranked, int(r[\"target\"]), k))\n",
        "        p, r_, f1 = precision_recall_f1(ranked, int(r[\"target\"]), k)\n",
        "        precs.append(p); recs.append(r_); f1s.append(f1)\n",
        "    return (\n",
        "        float(np.mean(hits)), float(np.mean(ndcgs)),\n",
        "        float(np.mean(precs)), float(np.mean(recs)), float(np.mean(f1s))\n",
        "    )\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Training + evaluation function\n",
        "# ------------------------------------------------------\n",
        "\n",
        "def train_eval_als_once(seed, R, alpha, factors, reg, iters, use_gpu,\n",
        "                        val_eval, test_eval, k=10):\n",
        "    X = bm25_weight(R, K1=100, B=0.8).astype(np.float32)\n",
        "    als = AlternatingLeastSquares(\n",
        "        factors=factors,\n",
        "        regularization=reg,\n",
        "        iterations=iters,\n",
        "        use_gpu=use_gpu,\n",
        "        dtype=np.float32,\n",
        "        random_state=seed,\n",
        "    )\n",
        "    als.fit(alpha * X)\n",
        "\n",
        "    U_f = als.user_factors\n",
        "    V_f = als.item_factors\n",
        "    I   = V_f.shape[0]\n",
        "\n",
        "    def score_als(uid, cand_iids):\n",
        "        if not cand_iids: return np.asarray([])\n",
        "        u = U_f[int(uid)]\n",
        "        c = np.array([int(i) for i in cand_iids if 0 <= int(i) < I], dtype=np.int64)\n",
        "        if c.size == 0: return np.asarray([])\n",
        "        return V_f[c] @ u\n",
        "\n",
        "    return eval_pool(val_eval,  score_als, k), eval_pool(test_eval, score_als, k)\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Grid search over hyperparameters\n",
        "# ------------------------------------------------------\n",
        "\n",
        "param_grid = [\n",
        "    {\"factors\": 64,  \"reg\": 0.05, \"alpha\": 15, \"iters\": 150},\n",
        "    {\"factors\": 96,  \"reg\": 0.05, \"alpha\": 15, \"iters\": 200},\n",
        "    {\"factors\": 128, \"reg\": 0.1,  \"alpha\": 10, \"iters\": 200},\n",
        "    {\"factors\": 128, \"reg\": 0.05, \"alpha\": 20, \"iters\": 200},\n",
        "]\n",
        "\n",
        "results = []\n",
        "seeds = [0, 1, 2]  # average across a few random seeds\n",
        "\n",
        "for params in param_grid:\n",
        "    vals, tests = [], []\n",
        "    for s in seeds:\n",
        "        val_metrics, test_metrics = train_eval_als_once(\n",
        "            seed=s,\n",
        "            R=R,\n",
        "            alpha=params[\"alpha\"],\n",
        "            factors=params[\"factors\"],\n",
        "            reg=params[\"reg\"],\n",
        "            iters=params[\"iters\"],\n",
        "            use_gpu=False,\n",
        "            val_eval=val_eval,\n",
        "            test_eval=test_eval,\n",
        "            k=10,\n",
        "        )\n",
        "        vals.append(val_metrics)\n",
        "        tests.append(test_metrics)\n",
        "\n",
        "    vals, tests = np.array(vals), np.array(tests)\n",
        "    val_mean, val_std = vals.mean(0), vals.std(0)\n",
        "    test_mean, test_std = tests.mean(0), tests.std(0)\n",
        "\n",
        "    results.append({\n",
        "        **params,\n",
        "        \"val_HR\": val_mean[0], \"val_NDCG\": val_mean[1],\n",
        "        \"val_P\": val_mean[2],  \"val_R\": val_mean[3],  \"val_F1\": val_mean[4],\n",
        "        \"test_HR\": test_mean[0], \"test_NDCG\": test_mean[1],\n",
        "        \"test_P\": test_mean[2],  \"test_R\": test_mean[3], \"test_F1\": test_mean[4],\n",
        "    })\n",
        "\n",
        "# ------------------------------------------------------\n",
        "# Results summary\n",
        "# ------------------------------------------------------\n",
        "\n",
        "res_df = pd.DataFrame(results)\n",
        "print(res_df.sort_values(\"val_NDCG\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Best ALS hyperparameters: factors=128, regularization=0.10, alpha=10, iterations=200"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMhHE0NBlH/BSXrGjJvquND",
      "include_colab_link": true,
      "mount_file_id": "1v14zduS8ZnCfMpMoMMIvkrRdQZqD7Ei0",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ece1508gp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
