{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T01:32:44.787169Z",
     "start_time": "2025-11-18T01:32:44.768085Z"
    }
   },
   "source": [
    "import os, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\")\n",
    "\n",
    "for d in [\"splits\", \"candidates\"]:\n",
    "    p = BASE / d\n",
    "    if p.exists():\n",
    "        shutil.rmtree(p)\n",
    "        print(\"[clean] removed\", p)\n",
    "    else:\n",
    "        print(\"[clean] not found\", p)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[clean] removed C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\splits\n",
      "[clean] removed C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\candidates\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T01:34:40.776630Z",
     "start_time": "2025-11-18T01:32:44.795732Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "CSV_PATH = BASE / \"Books_rating.csv\"\n",
    "assert CSV_PATH.exists(), f\"Missing file: {CSV_PATH}\"\n",
    "\n",
    "df_raw = pd.read_csv(CSV_PATH)\n",
    "print(\"Raw columns:\", list(df_raw.columns))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"userId\":    df_raw[\"User_id\"],\n",
    "    \"itemId\":    df_raw[\"Id\"],\n",
    "    \"rating\":    pd.to_numeric(df_raw[\"review/score\"], errors=\"coerce\"),\n",
    "    \"timestamp\": pd.to_datetime(df_raw[\"review/time\"], errors=\"coerce\"),\n",
    "})\n",
    "\n",
    "df[\"timestamp\"] = df[\"timestamp\"].fillna(pd.Timestamp(2000,1,1))\n",
    "df[\"timestamp\"] = (df[\"timestamp\"].astype(\"int64\") // 10**9)\n",
    "\n",
    "df = df[(df[\"rating\"] >= 1) & (df[\"rating\"] <= 5)]\n",
    "\n",
    "print(df.head(), df.dtypes)\n",
    "\n",
    "def kcore_filter(df, u_col=\"userId\", i_col=\"itemId\", k_user=5, k_item=5, max_iters=20, verbose=True):\n",
    "    for it in range(max_iters):\n",
    "        n0, u0, i0 = len(df), df[u_col].nunique(), df[i_col].nunique()\n",
    "        uf = df[u_col].value_counts()\n",
    "        vf = df[i_col].value_counts()\n",
    "        df = df[df[u_col].isin(uf[uf >= k_user].index)]\n",
    "        df = df[df[i_col].isin(vf[vf >= k_item].index)]\n",
    "        n1, u1, i1 = len(df), df[u_col].nunique(), df[i_col].nunique()\n",
    "        if verbose:\n",
    "            print(f\"[k-core {it+1}] rows {n0:,}->{n1:,}, users {u0:,}->{u1:,}, items {i0:,}->{i1:,}\")\n",
    "        if n1 == n0:\n",
    "            break\n",
    "    return df\n",
    "\n",
    "df = kcore_filter(df, k_user=5, k_item=5)\n",
    "print(f\"[after k-core] users={df['userId'].nunique():,}, items={df['itemId'].nunique():,}, rows={len(df):,}\")\n",
    "\n",
    "ratings_csv = BASE / \"ratings.csv\"\n",
    "df.to_csv(ratings_csv, index=False)\n",
    "print(f\"[saved] {ratings_csv} ~ {len(df):,} rows\")"
   ],
   "id": "c1f1e456a8558257",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw columns: ['Id', 'Title', 'Price', 'User_id', 'profileName', 'review/helpfulness', 'review/score', 'review/time', 'review/summary', 'review/text']\n",
      "           userId      itemId  rating  timestamp\n",
      "0   AVCGYZL8FQQTD  1882931173     4.0          0\n",
      "1  A30TK6U7DNS82R  0826414346     5.0          1\n",
      "2  A3UH4UZ4RSVO82  0826414346     5.0          1\n",
      "3  A2MVUWT453QH61  0826414346     4.0          1\n",
      "4  A22X4XUPKF66MR  0826414346     4.0          1 userId        object\n",
      "itemId        object\n",
      "rating       float64\n",
      "timestamp      int64\n",
      "dtype: object\n",
      "[k-core 1] rows 3,000,000->1,077,091, users 1,008,972->82,519, items 221,998->69,986\n",
      "[k-core 2] rows 1,077,091->977,301, users 82,519->77,225, items 69,986->29,668\n",
      "[k-core 3] rows 977,301->951,522, users 77,225->70,381, items 29,668->28,742\n",
      "[k-core 4] rows 951,522->944,212, users 70,381->70,121, items 28,742->27,027\n",
      "[k-core 5] rows 944,212->941,724, users 70,121->69,534, items 27,027->26,952\n",
      "[k-core 6] rows 941,724->940,863, users 69,534->69,496, items 26,952->26,770\n",
      "[k-core 7] rows 940,863->940,525, users 69,496->69,417, items 26,770->26,757\n",
      "[k-core 8] rows 940,525->940,434, users 69,417->69,416, items 26,757->26,731\n",
      "[k-core 9] rows 940,434->940,416, users 69,416->69,411, items 26,731->26,731\n",
      "[k-core 10] rows 940,416->940,416, users 69,411->69,411, items 26,731->26,731\n",
      "[after k-core] users=69,411, items=26,731, rows=940,416\n",
      "[saved] C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\ratings.csv ~ 940,416 rows\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T01:34:41.109619Z",
     "start_time": "2025-11-18T01:34:40.994120Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def time_aware_loo_split(\n",
    "    ratings_csv: str,\n",
    "    out_dir: str,\n",
    "    rating_threshold: float = 4.0,\n",
    "    min_positives: int = 2,\n",
    "    also_csv: bool = False,\n",
    "):\n",
    "    out = Path(out_dir); (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
    "    ratings = pd.read_csv(ratings_csv)\n",
    "\n",
    "    need = {\"userId\",\"itemId\",\"rating\",\"timestamp\"}\n",
    "    missing = need - set(ratings.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
    "\n",
    "    unit = \"ms\" if float(ratings[\"timestamp\"].max()) > 1e12 else \"s\"\n",
    "    ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
    "\n",
    "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
    "\n",
    "    pos = pos.drop_duplicates([\"userId\",\"itemId\"], keep=\"first\")\n",
    "\n",
    "    cnt = pos.groupby(\"userId\")[\"itemId\"].transform(\"size\")\n",
    "    pos = pos[cnt >= min_positives].copy()\n",
    "\n",
    "    pos = pos.sort_values([\"userId\",\"ts\"], kind=\"mergesort\")\n",
    "    pos[\"n\"]   = pos.groupby(\"userId\")[\"userId\"].transform(\"size\")\n",
    "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
    "    pos[\"split\"] = \"train\"\n",
    "    pos.loc[pos[\"idx\"] == pos[\"n\"]-1, \"split\"] = \"test\"\n",
    "    pos.loc[(pos[\"n\"] >= 3) & (pos[\"idx\"] == pos[\"n\"]-2), \"split\"] = \"val\"\n",
    "\n",
    "    train = pos[pos[\"split\"]==\"train\"][[\"userId\",\"itemId\",\"ts\"]].reset_index(drop=True)\n",
    "    val_targets  = (pos[pos[\"split\"]==\"val\"][[\"userId\",\"itemId\",\"ts\"]]\n",
    "                    .rename(columns={\"itemId\":\"val_item\",\"ts\":\"ts_val\"}).reset_index(drop=True))\n",
    "    test_targets = (pos[pos[\"split\"]==\"test\"][[\"userId\",\"itemId\",\"ts\"]]\n",
    "                    .rename(columns={\"itemId\":\"test_item\",\"ts\":\"ts_test\"}).reset_index(drop=True))\n",
    "\n",
    "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"]); uids[\"uid\"] = range(len(uids))\n",
    "    iids = pd.DataFrame(sorted(train[\"itemId\"].unique()), columns=[\"itemId\"]); iids[\"iid\"] = range(len(iids))\n",
    "\n",
    "    val_idx  = (val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "                          .merge(iids, left_on=\"val_item\", right_on=\"itemId\", how=\"left\")\n",
    "                          .drop(columns=[\"itemId\"]))\n",
    "    test_idx = (test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "                          .merge(iids, left_on=\"test_item\", right_on=\"itemId\", how=\"left\")\n",
    "                          .drop(columns=[\"itemId\"]))\n",
    "\n",
    "    sp = out / \"splits\"\n",
    "    train.to_parquet(sp / \"train.parquet\", index=False)\n",
    "    if len(val_targets):  val_targets.to_parquet(sp / \"val_targets.parquet\", index=False)\n",
    "    test_targets.to_parquet(sp / \"test_targets.parquet\", index=False)\n",
    "    uids.to_parquet(sp / \"user_id_map.parquet\", index=False)\n",
    "    iids.to_parquet(sp / \"item_id_map.parquet\", index=False)\n",
    "    (train.merge(uids, on=\"userId\", how=\"inner\")\n",
    "          .merge(iids, on=\"itemId\", how=\"inner\")\n",
    "          .drop(columns=[\"userId\",\"itemId\"])\n",
    "          .to_parquet(sp / \"train_indexed.parquet\", index=False))\n",
    "    if len(val_idx):   val_idx.to_parquet(sp / \"val_targets_indexed.parquet\", index=False)\n",
    "    test_idx.to_parquet(sp / \"test_targets_indexed.parquet\", index=False)\n",
    "\n",
    "    cold_val  = int(val_idx[\"iid\"].isna().sum()) if len(val_idx) else 0\n",
    "    cold_test = int(test_idx[\"iid\"].isna().sum()) if \"iid\" in test_idx else 0\n",
    "    stats = f\"\"\"Time-aware LOO split summary\n",
    "Users (TRAIN map): {len(uids):,}\n",
    "Items (TRAIN map): {len(iids):,}\n",
    "TRAIN positives : {len(train):,}\n",
    "VAL users       : {val_idx[\"uid\"].nunique() if len(val_idx) else 0:,}\n",
    "TEST users      : {test_idx[\"uid\"].nunique() if len(test_idx) else 0:,}\n",
    "Cold-start VAL items : {cold_val}\n",
    "Cold-start TEST items: {cold_test}\n",
    "\"\"\"\n",
    "    (sp / \"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
    "    print(stats)"
   ],
   "id": "e14a172ead2843a2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:34:48.922891Z",
     "start_time": "2025-11-18T01:34:41.122525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _detect_ts_unit(ts_series):\n",
    "    vmax = float(ts_series.max())\n",
    "    return \"ms\" if vmax > 1e12 else \"s\"\n",
    "\n",
    "def time_aware_loo_split(\n",
    "    ratings_csv: str,\n",
    "    out_dir: str,\n",
    "    rating_threshold: float = 4.0,\n",
    "    min_positives: int = 2,\n",
    "    also_csv: bool = False,\n",
    "):\n",
    "    out = Path(out_dir); (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
    "    ratings = pd.read_csv(ratings_csv)\n",
    "\n",
    "    need = {\"userId\",\"itemId\",\"rating\",\"timestamp\"}\n",
    "    missing = need - set(ratings.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
    "\n",
    "    unit = _detect_ts_unit(ratings[\"timestamp\"])\n",
    "    ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
    "\n",
    "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
    "    pos = pos.sort_values([\"userId\",\"ts\"], kind=\"mergesort\")\n",
    "\n",
    "    pos = ratings.loc[ratings[\"rating\"] >= rating_threshold,\n",
    "                      [\"userId\", \"itemId\", \"timestamp\"]].copy()\n",
    "\n",
    "    pos = pos.drop_duplicates([\"userId\", \"itemId\"], keep=\"first\")\n",
    "\n",
    "    ts_unit = \"ms\" if float(pos[\"timestamp\"].max()) > 1e12 else \"s\"\n",
    "    pos[\"ts\"] = pd.to_datetime(pos[\"timestamp\"], unit=ts_unit, errors=\"coerce\")\n",
    "\n",
    "    cnt = pos.groupby(\"userId\")[\"itemId\"].transform(\"size\")\n",
    "    pos = pos[cnt >= min_positives].copy()\n",
    "\n",
    "    pos = pos.sort_values([\"userId\", \"ts\"], kind=\"mergesort\")\n",
    "    pos[\"n\"]   = pos.groupby(\"userId\")[\"itemId\"].transform(\"size\")\n",
    "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
    "    pos[\"split\"] = \"train\"\n",
    "    pos.loc[pos[\"idx\"] == pos[\"n\"] - 1, \"split\"] = \"test\"\n",
    "    pos.loc[(pos[\"n\"] >= 3) & (pos[\"idx\"] == pos[\"n\"] - 2), \"split\"] = \"val\"\n",
    "\n",
    "    pos = pos.sort_values([\"userId\",\"ts\"], kind=\"mergesort\")\n",
    "    pos[\"n\"]   = pos.groupby(\"userId\")[\"userId\"].transform(\"size\")\n",
    "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
    "    pos[\"split\"] = \"train\"\n",
    "    pos.loc[pos[\"idx\"] == pos[\"n\"]-1, \"split\"] = \"test\"\n",
    "    pos.loc[(pos[\"n\"] >= 3) & (pos[\"idx\"] == pos[\"n\"]-2), \"split\"] = \"val\"\n",
    "\n",
    "    train = pos[pos[\"split\"]==\"train\"][[\"userId\",\"itemId\",\"ts\"]].reset_index(drop=True)\n",
    "    val_targets  = pos[pos[\"split\"]==\"val\"][[\"userId\",\"itemId\",\"ts\"]].rename(columns={\"itemId\":\"val_item\",\"ts\":\"ts_val\"}).reset_index(drop=True)\n",
    "    test_targets = pos[pos[\"split\"]==\"test\"][[\"userId\",\"itemId\",\"ts\"]].rename(columns={\"itemId\":\"test_item\",\"ts\":\"ts_test\"}).reset_index(drop=True)\n",
    "\n",
    "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"])\n",
    "    uids[\"uid\"] = range(len(uids))\n",
    "    iids = pd.DataFrame(sorted(train[\"itemId\"].unique()), columns=[\"itemId\"])\n",
    "    iids[\"iid\"] = range(len(iids))\n",
    "\n",
    "    val_idx  = (val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "                          .merge(iids, left_on=\"val_item\", right_on=\"itemId\", how=\"left\")\n",
    "                          .drop(columns=[\"itemId\"]))\n",
    "    test_idx = (test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "                          .merge(iids, left_on=\"test_item\", right_on=\"itemId\", how=\"left\")\n",
    "                          .drop(columns=[\"itemId\"]))\n",
    "\n",
    "    sp = out / \"splits\"\n",
    "    train.to_parquet(sp / \"train.parquet\", index=False)\n",
    "    if len(val_targets):  val_targets.to_parquet(sp / \"val_targets.parquet\", index=False)\n",
    "    test_targets.to_parquet(sp / \"test_targets.parquet\", index=False)\n",
    "    uids.to_parquet(sp / \"user_id_map.parquet\", index=False)\n",
    "    iids.to_parquet(sp / \"item_id_map.parquet\", index=False)\n",
    "    (train.merge(uids, on=\"userId\", how=\"inner\")\n",
    "          .merge(iids, on=\"itemId\", how=\"inner\")\n",
    "          .drop(columns=[\"userId\",\"itemId\"])\n",
    "          .to_parquet(sp / \"train_indexed.parquet\", index=False))\n",
    "    if len(val_idx):   val_idx.to_parquet(sp / \"val_targets_indexed.parquet\", index=False)\n",
    "    test_idx.to_parquet(sp / \"test_targets_indexed.parquet\", index=False)\n",
    "\n",
    "    cold_val  = int(val_idx[\"iid\"].isna().sum()) if len(val_idx) else 0\n",
    "    cold_test = int(test_idx[\"iid\"].isna().sum()) if \"iid\" in test_idx else 0\n",
    "    stats = f\"\"\"Time-aware LOO split summary\n",
    "Users (TRAIN map): {len(uids):,}\n",
    "Items (TRAIN map): {len(iids):,}\n",
    "TRAIN positives : {len(train):,}\n",
    "VAL users       : {val_idx[\"uid\"].nunique() if len(val_idx) else 0:,}\n",
    "TEST users      : {test_idx[\"uid\"].nunique() if len(test_idx) else 0:,}\n",
    "Cold-start VAL items : {cold_val}\n",
    "Cold-start TEST items: {cold_test}\n",
    "\"\"\"\n",
    "    (sp / \"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
    "    print(stats)\n",
    "\n",
    "time_aware_loo_split(\n",
    "    ratings_csv=str(BASE/\"ratings.csv\"),\n",
    "    out_dir=str(BASE),\n",
    "    rating_threshold=3.0,\n",
    "    min_positives=3,\n",
    "    also_csv=False,\n",
    ")"
   ],
   "id": "6f4f56866967c54a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-aware LOO split summary\n",
      "Users (TRAIN map): 64,541\n",
      "Items (TRAIN map): 26,497\n",
      "TRAIN positives : 697,181\n",
      "VAL users       : 64,541\n",
      "TEST users      : 64,541\n",
      "Cold-start VAL items : 257\n",
      "Cold-start TEST items: 1078\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T01:34:58.643020Z",
     "start_time": "2025-11-18T01:34:48.933255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "def _detect_ts_unit(ts_series: pd.Series) -> str:\n",
    "    vmax = float(ts_series.max())\n",
    "    return \"ms\" if vmax > 1e12 else \"s\"\n",
    "\n",
    "def time_aware_loo_split(\n",
    "    ratings_csv: str,\n",
    "    out_dir: str,\n",
    "    rating_threshold: float = 3.0,\n",
    "    min_positives: int = 3,\n",
    "    also_csv: bool = False,\n",
    "):\n",
    "    out = Path(out_dir); (out / \"splits\").mkdir(parents=True, exist_ok=True)\n",
    "    ratings = pd.read_csv(ratings_csv)\n",
    "\n",
    "    need = {\"userId\",\"itemId\",\"rating\",\"timestamp\"}\n",
    "    missing = need - set(ratings.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"ratings.csv missing columns: {missing}\")\n",
    "\n",
    "\n",
    "    try:\n",
    "        unit = _detect_ts_unit(ratings[\"timestamp\"])\n",
    "        ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=unit)\n",
    "    except Exception:\n",
    "\n",
    "        ratings[\"ts\"] = pd.to_datetime(ratings[\"timestamp\"], unit=\"s\", origin=\"unix\", errors=\"ignore\")\n",
    "\n",
    "    pos = ratings[ratings[\"rating\"] >= rating_threshold].copy()\n",
    "    pos = pos.sort_values([\"userId\",\"itemId\",\"ts\"], kind=\"mergesort\").drop_duplicates([\"userId\",\"itemId\"], keep=\"first\")\n",
    "    pos = pos.groupby(\"userId\").filter(lambda g: len(g) >= min_positives)\n",
    "\n",
    "    pos = pos.sort_values([\"userId\",\"ts\"], kind=\"mergesort\")\n",
    "    pos[\"n\"]   = pos.groupby(\"userId\")[\"itemId\"].transform(\"size\")\n",
    "    pos[\"idx\"] = pos.groupby(\"userId\").cumcount()\n",
    "\n",
    "    pos[\"split\"] = \"train\"\n",
    "    pos.loc[pos[\"idx\"]==pos[\"n\"]-1, \"split\"] = \"test\"\n",
    "    pos.loc[(pos[\"n\"]>=3) & (pos[\"idx\"]==pos[\"n\"]-2), \"split\"] = \"val\"\n",
    "\n",
    "    train = pos[pos[\"split\"]==\"train\"][[\"userId\",\"itemId\",\"ts\"]].reset_index(drop=True)\n",
    "    val_targets  = pos[pos[\"split\"]==\"val\"][[\"userId\",\"itemId\",\"ts\"]].rename(columns={\"itemId\":\"val_item\",\"ts\":\"ts_val\"}).reset_index(drop=True)\n",
    "    test_targets = pos[pos[\"split\"]==\"test\"][[\"userId\",\"itemId\",\"ts\"]].rename(columns={\"itemId\":\"test_item\",\"ts\":\"ts_test\"}).reset_index(drop=True)\n",
    "\n",
    "    uids = pd.DataFrame(sorted(train[\"userId\"].unique()), columns=[\"userId\"]); uids[\"uid\"]=range(len(uids))\n",
    "    iids = pd.DataFrame(sorted(train[\"itemId\"].unique()), columns=[\"itemId\"]); iids[\"iid\"]=range(len(iids))\n",
    "\n",
    "    train_idx = (train.merge(uids, on=\"userId\").merge(iids, on=\"itemId\"))\n",
    "    val_idx   = (val_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "                           .merge(iids, left_on=\"val_item\", right_on=\"itemId\", how=\"left\")\n",
    "                           .drop(columns=[\"itemId\"]))\n",
    "    test_idx  = (test_targets.merge(uids, on=\"userId\", how=\"inner\")\n",
    "                             .merge(iids, left_on=\"test_item\", right_on=\"itemId\", how=\"left\")\n",
    "                             .drop(columns=[\"itemId\"]))\n",
    "\n",
    "    sp = Path(out_dir) / \"splits\"\n",
    "    train.to_parquet(sp/\"train.parquet\", index=False)\n",
    "    val_targets.to_parquet(sp/\"val_targets.parquet\", index=False)\n",
    "    test_targets.to_parquet(sp/\"test_targets.parquet\", index=False)\n",
    "    uids.to_parquet(sp/\"user_id_map.parquet\", index=False)\n",
    "    iids.to_parquet(sp/\"item_id_map.parquet\", index=False)\n",
    "    train_idx.to_parquet(sp/\"train_indexed.parquet\", index=False)\n",
    "    val_idx.to_parquet(sp/\"val_targets_indexed.parquet\", index=False)\n",
    "    test_idx.to_parquet(sp/\"test_targets_indexed.parquet\", index=False)\n",
    "\n",
    "    if also_csv:\n",
    "        for p in [\"train\",\"val_targets\",\"test_targets\",\"user_id_map\",\"item_id_map\",\"train_indexed\",\"val_targets_indexed\",\"test_targets_indexed\"]:\n",
    "            pd.read_parquet(sp/f\"{p}.parquet\").to_csv(sp/f\"{p}.csv\", index=False)\n",
    "\n",
    "    cold_val  = int(val_idx[\"iid\"].isna().sum()) if \"iid\" in val_idx.columns else 0\n",
    "    cold_test = int(test_idx[\"iid\"].isna().sum()) if \"iid\" in test_idx.columns else 0\n",
    "    stats = f\"\"\"Time-aware LOO split summary\n",
    "Users (TRAIN map): {len(uids)}\n",
    "Items (TRAIN map): {len(iids)}\n",
    "TRAIN positives : {len(train)}\n",
    "VAL users       : {len(val_targets['userId'].unique())}\n",
    "TEST users      : {len(test_targets['userId'].unique())}\n",
    "Cold-start VAL items : {cold_val}\n",
    "Cold-start TEST items: {cold_test}\n",
    "\"\"\"\n",
    "    (sp/\"stats.txt\").write_text(stats, encoding=\"utf-8\")\n",
    "    print(stats)\n",
    "\n",
    "# Run split on the normalized Kaggle CSV\n",
    "time_aware_loo_split(\n",
    "    ratings_csv=str(BASE/\"ratings.csv\"),\n",
    "    out_dir=str(BASE),\n",
    "    rating_threshold=3.0,\n",
    "    min_positives=3,\n",
    "    also_csv=False\n",
    ")"
   ],
   "id": "8319054b873fdc3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time-aware LOO split summary\n",
      "Users (TRAIN map): 64541\n",
      "Items (TRAIN map): 26531\n",
      "TRAIN positives : 697181\n",
      "VAL users       : 64541\n",
      "TEST users      : 64541\n",
      "Cold-start VAL items : 240\n",
      "Cold-start TEST items: 1049\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:34:59.834081Z",
     "start_time": "2025-11-18T01:34:58.656824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "SPLITS = BASE / \"splits\"\n",
    "train_idx = pd.read_parquet(SPLITS/\"train_indexed.parquet\")\n",
    "val_idx   = pd.read_parquet(SPLITS/\"val_targets_indexed.parquet\")\n",
    "test_idx  = pd.read_parquet(SPLITS/\"test_targets_indexed.parquet\")\n",
    "\n",
    "train_items = set(train_idx[\"iid\"].unique())\n",
    "val_keep  = val_idx[val_idx[\"iid\"].isin(train_items)].copy()\n",
    "test_keep = test_idx[test_idx[\"iid\"].isin(train_items)].copy()\n",
    "\n",
    "val_keep.to_parquet(SPLITS/\"val_targets_indexed.parquet\", index=False)\n",
    "test_keep.to_parquet(SPLITS/\"test_targets_indexed.parquet\", index=False)\n",
    "print(\"[covered] kept val:\", len(val_keep), \" / test:\", len(test_keep))\n"
   ],
   "id": "5458cf22cb944ff6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[covered] kept val: 64301  / test: 63492\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-18T01:35:02.381258Z",
     "start_time": "2025-11-18T01:34:59.850004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "SPLITS = BASE / \"splits\"\n",
    "train_idx = pd.read_parquet(SPLITS/\"train_indexed.parquet\")   # [uid, iid, ts]\n",
    "val_idx   = pd.read_parquet(SPLITS/\"val_targets_indexed.parquet\")   # [uid, val_item(iid), ts_val]\n",
    "test_idx  = pd.read_parquet(SPLITS/\"test_targets_indexed.parquet\")  # [uid, test_item(iid), ts_test]\n",
    "\n",
    "U = int(train_idx[\"uid\"].max()) + 1\n",
    "I = int(train_idx[\"iid\"].max()) + 1\n",
    "\n",
    "user_seen = train_idx.groupby(\"uid\")[\"iid\"].apply(set).to_dict()\n",
    "\n",
    "def candidate_coverage(cand_df, targets_df, tgt_col=\"iid\"):\n",
    "    df = cand_df.merge(targets_df[[\"uid\", tgt_col]], on=\"uid\", how=\"inner\")\n",
    "    df = df[df[tgt_col].notna()]\n",
    "    return np.mean([(int(t) in set(c)) for t, c in zip(df[tgt_col], df[\"candidates\"])])"
   ],
   "id": "ea1dd888e565b82f",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:35:06.304327Z",
     "start_time": "2025-11-18T01:35:02.398893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_idx_sorted = train_idx.sort_values([\"uid\",\"ts\"]).groupby(\"uid\").tail(200)\n",
    "\n",
    "R = csr_matrix(\n",
    "    (np.ones(len(train_idx_sorted), dtype=np.float32),\n",
    "     (train_idx_sorted[\"uid\"].astype(int).values,\n",
    "      train_idx_sorted[\"iid\"].astype(int).values)),\n",
    "    shape=(U, I),\n",
    "    dtype=np.float32\n",
    ")\n",
    "\n",
    "C = (R.T @ R).tocsr()\n",
    "C.setdiag(0); C.eliminate_zeros()\n",
    "\n",
    "M_SIM = 50\n",
    "item_sim_map = {}\n",
    "for iid in range(I):\n",
    "    a, b = C.indptr[iid], C.indptr[iid+1]\n",
    "    if a == b:\n",
    "        item_sim_map[iid] = []\n",
    "        continue\n",
    "    neigh = C.indices[a:b]\n",
    "    vals  = C.data[a:b]\n",
    "    if len(neigh) > M_SIM:\n",
    "        top = np.argpartition(-vals, M_SIM)[:M_SIM]\n",
    "        neigh, vals = neigh[top], vals[top]\n",
    "    order = np.argsort(-vals)\n",
    "    item_sim_map[iid] = neigh[order].tolist()\n",
    "\n",
    "print(f\"[item-sim] built for I={I:,}. Example of item 0:\", item_sim_map.get(0, [])[:10])"
   ],
   "id": "139b02f64124c0d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[item-sim] built for I=26,531. Example of item 0: [22824, 8, 8131, 20798, 486, 14822, 22726, 22580, 13332, 4209]\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:38:18.988504Z",
     "start_time": "2025-11-18T01:35:06.316828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "item_pop_series = train_idx['iid'].value_counts().astype(float)\n",
    "item_pop_series = item_pop_series / item_pop_series.max()\n",
    "item_pop = item_pop_series.to_dict()\n",
    "item_popular = sorted(item_pop.items(), key=lambda x: -x[1])  # 兜底\n",
    "\n",
    "def score_items_for_user(uid, recent=15, sim_per_item=50, alpha=1.0, beta=0.2):\n",
    "\n",
    "    seen = user_seen.get(uid, set())\n",
    "    recent_items = (train_idx.loc[train_idx[\"uid\"]==uid]\n",
    "                             .sort_values(\"ts\", ascending=False)\n",
    "                             .head(recent)[\"iid\"].astype(int).tolist())\n",
    "    scores = {}\n",
    "    for j in recent_items:\n",
    "        for i in item_sim_map.get(j, [])[:sim_per_item]:\n",
    "            if i in seen:\n",
    "                continue\n",
    "            scores[i] = scores.get(i, 0.0) + alpha\n",
    "    for i in list(scores.keys())[:5000]:\n",
    "        scores[i] += beta * item_pop.get(int(i), 0.0)\n",
    "    return scores\n",
    "\n",
    "def build_pool_for_user(uid, K=50):\n",
    "    seen = user_seen.get(uid, set())\n",
    "    scores = score_items_for_user(uid, recent=15, sim_per_item=50, alpha=1.0, beta=0.2)\n",
    "    if not scores:\n",
    "        pop_list = [int(i) for i,_ in item_popular if int(i) not in seen]\n",
    "        return pop_list[:K]\n",
    "    top = sorted(scores.items(), key=lambda x: -x[1])[:K]\n",
    "    return [int(i) for i,_ in top]\n",
    "\n",
    "def make_candidates(user_ids, K=50):\n",
    "    rows = []\n",
    "    for u in tqdm(user_ids):\n",
    "        rows.append({\"uid\": int(u), \"candidates\": build_pool_for_user(int(u), K=K)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "users_val  = sorted(val_idx[\"uid\"].unique())\n",
    "users_test = sorted(test_idx[\"uid\"].unique())\n",
    "\n",
    "cand_val  = make_candidates(users_val,  K=50)\n",
    "cand_test = make_candidates(users_test, K=50)\n",
    "\n",
    "OUT = BASE / \"candidates\"; OUT.mkdir(parents=True, exist_ok=True)\n",
    "cand_val.to_parquet(OUT/\"val.parquet\",  index=False)\n",
    "cand_test.to_parquet(OUT/\"test.parquet\", index=False)\n",
    "print(\"[saved candidates] ->\", OUT)"
   ],
   "id": "3c31f1680ec89641",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64301/64301 [01:47<00:00, 600.38it/s]\n",
      "100%|██████████| 63492/63492 [01:24<00:00, 752.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved candidates] -> C:\\Users\\carlk\\OneDrive\\Documents\\uoft\\ECE1508H F\\Project\\candidates\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T01:38:19.853927Z",
     "start_time": "2025-11-18T01:38:19.068826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cand_val  = pd.read_parquet(BASE/\"candidates/val.parquet\")\n",
    "cand_test = pd.read_parquet(BASE/\"candidates/test.parquet\")\n",
    "\n",
    "has_iid_val  = \"iid\" in val_idx.columns\n",
    "has_iid_test = \"iid\" in test_idx.columns\n",
    "print(\"val has iid:\", has_iid_val, \"test has iid:\", has_iid_test)\n",
    "\n",
    "val_cov  = candidate_coverage(cand_val,  val_idx,  \"iid\") if has_iid_val  else float(\"nan\")\n",
    "test_cov = candidate_coverage(cand_test, test_idx, \"iid\") if has_iid_test else float(\"nan\")\n",
    "print(f\"Candidate coverage  val={val_cov:.2%}  test={test_cov:.2%}\")"
   ],
   "id": "4edfe4b220fdeef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val has iid: True test has iid: True\n",
      "Candidate coverage  val=62.19%  test=59.01%\n"
     ]
    }
   ],
   "execution_count": 21
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
